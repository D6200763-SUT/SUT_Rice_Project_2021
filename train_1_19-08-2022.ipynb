{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import library python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "import datetime,os\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "# import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# import tensorflow \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "# import platform\n",
    "# print(platform.python_version())\n",
    "# print(np.__version__)\n",
    "# print(tf.__version__)\n",
    "\n",
    "# tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All low RiceCenter 34 station\n"
     ]
    }
   ],
   "source": [
    "# File name and Path\n",
    "cwd = os.getcwd()\n",
    "path_adress = \"/Import_Dataset/\"\n",
    "\n",
    "# ข้อมูล พิกัดสถานที่เเละรายชื้ออ้างอิงสถานที่ตรวจวัด\n",
    "file_name_st = 'Data_lat_long_Rice research Center'\n",
    "csv_file_st = cwd + path_adress + file_name_st + '.csv'\n",
    "\n",
    "df_st = pd.read_csv(csv_file_st)\n",
    "print(\"All low RiceCenter {} station\" .format(df_st.shape[0]))\n",
    "\n",
    "data_list_st_num = list(range(0, len(df_st)))\n",
    "data_list_st_name = []\n",
    "data_list_st_name = df_st['nameEng'].values.tolist()\n",
    "data_list_st_lat = df_st['Latitude'].values.tolist()\n",
    "data_list_st_long = df_st['Longitude'].values.tolist()\n",
    "\n",
    "#################-- Moving Averag data --#################\n",
    "def mavr_dataset(frames_sma,rolling_num,our_rate,mode):    \n",
    "    h_name = list(frames_sma)\n",
    "    if mode == 'sum':\n",
    "        frames_sma['mirid bug'] = frames_sma['mirid bug'].rolling(rolling_num, min_periods=1).sum().round(1)\n",
    "        frames_sma['bph'] = frames_sma['bph'].rolling(rolling_num, min_periods=1).sum().round(1)\n",
    "        frames_sma['precip'] = frames_sma['precip'].rolling(rolling_num, min_periods=1).sum().round(1)\n",
    "    else:\n",
    "        frames_sma['mirid bug'] = frames_sma['mirid bug'].rolling(rolling_num, min_periods=1).mean().round(1)\n",
    "        frames_sma['bph'] = frames_sma['bph'].rolling(rolling_num, min_periods=1).mean().round(1)\n",
    "        frames_sma['precip'] = frames_sma['precip'].rolling(rolling_num, min_periods=1).mean().round(1)\n",
    "   \n",
    "    for i in range(7,12):\n",
    "        frames_sma[h_name[i]] = frames_sma[h_name[i]].rolling(rolling_num, min_periods=1).mean().round(1)\n",
    "\n",
    "    constant_subset = frames_sma[::our_rate] \n",
    "    return constant_subset\n",
    "\n",
    "#################-- creat_dataset  --#################\n",
    "def creat_dataset(st_BPH = 'ALL', y_1='2015', y_2='2019', m_avr='ALL'):\n",
    "    for i in range(len(data_list_st_num)):\n",
    "        st = i\n",
    "        file_name = df_st['nameEng'][data_list_st_num[st]]\n",
    "        name_input = file_name\n",
    "        name_locals = 'stN_' + file_name\n",
    "\n",
    "        # File name and Path\n",
    "        csv_file = cwd + path_adress + name_input + '.csv'\n",
    "        # index_col=0 , index_col=None\n",
    "        dataset = pd.read_csv(csv_file, header=0,\n",
    "                              index_col=0, encoding=\"TIS-620\")\n",
    "\n",
    "        if m_avr == '3-DAY-All-':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,3,1,'mean')\n",
    "        elif m_avr == '7-DAY-All':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,7,1,'mean')\n",
    "        elif m_avr == '14-DAY-All':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,14,1,'mean')\n",
    "        elif m_avr == '3-DAY-Sampling':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,3,3,'sum')\n",
    "        elif m_avr == '7-DAY-Sampling':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,7,7,'sum')\n",
    "        elif m_avr == '14-DAY-Sampling':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,14,14,'sum')\n",
    "        else :\n",
    "            locals()[name_locals] = dataset\n",
    "            # print(f'Dataframe name_station: {st+1 , name_input}')\n",
    "        print('wait......')\n",
    "        del dataset\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    date_start = y_1 + '-01' + '-01'\n",
    "    date_stop = y_2 + '-12' + '-31'\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "    if st_BPH != 'ALL' :\n",
    "        # file_name = df_st['nameEng'][data_list_st_num[st_BPH]]\n",
    "        file_name = st_BPH\n",
    "        locals_input = 'stN_' + file_name\n",
    "        print(locals_input)\n",
    "        dataset_st = locals()[locals_input].loc[date_start:date_stop]\n",
    "        frames_st = dataset_st\n",
    "    else:\n",
    "        m = 0\n",
    "        for j in range(len(data_list_st_num)):\n",
    "            # for j in range(df_st.shape[0]):\n",
    "            # file_name = df_st['nameEng'][j]\n",
    "            file_name = df_st['nameEng'][data_list_st_num[j]]\n",
    "            locals_input = 'stN_' + file_name\n",
    "            print(locals_input)\n",
    "            dataset_st = locals()[locals_input].loc[date_start:date_stop]\n",
    "            clear_output(wait=True)\n",
    "            if m == 0:\n",
    "                frames_st = dataset_st\n",
    "                m = m+1\n",
    "                print(m)\n",
    "            else:\n",
    "                frames_st = [frames_st, dataset_st]\n",
    "                frames_st = pd.concat(frames_st)\n",
    "\n",
    "    # dataset    \n",
    "    frames = frames_st\n",
    "    return frames\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#################-- Drop Colum  --#################\n",
    "def drop_col(df_drop,drop = ['address']):\n",
    "    for j in range(len(drop)):\n",
    "        df_drop = df_drop.drop(drop[j], axis=1)\n",
    "    return df_drop\n",
    "\n",
    "#################--      Plot        --#################\n",
    "def plot_data(frames_train,plot_cols,df_name,start='2015',end='2017',st = 'ALL'):\n",
    "    #ตรวจสอบข้อมูล dataset \n",
    "    if st == 'ALL':\n",
    "        df_plot = frames_train[plot_cols]\n",
    "    else:\n",
    "        df_plot = frames_train[plot_cols].loc[start:end]\n",
    "    \n",
    "    plt.figure()    \n",
    "    # df_plot.plot(lw=1,grid=True,figsize=(13,30),subplots=True)\n",
    "    # df_plot.plot(lw=1,grid=True,subplots=True)\n",
    "    # df_plot.plot(marker='.',grid=True,linestyle = 'solid',subplots=True)\n",
    "    df_plot.plot(marker='.',grid=True,linestyle = 'solid')\n",
    "    plt.xlabel('Date time-'+ df_name)\n",
    "    plt.legend()\n",
    "    # plt.show()   \n",
    "\n",
    "#################--   get list of folders in directory   --#################\n",
    "dir_path = cwd + \"/Export_lstm/model/\"\n",
    "def get_list_folder():\n",
    "    folder_list = os.listdir(dir_path)\n",
    "    return folder_list\n",
    "\n",
    "def get_file_model(file_name):\n",
    "    txt = dir_path+file_name+\"/Training_model_data_discription.txt\"\n",
    "    string_data = open(txt,\"r\").read()\n",
    "    list_txt = list(string_data.split(\"\\n\"))\n",
    "    return list_txt\n",
    "    \n",
    "  #################-- convert series to supervised learning --#################\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "\n",
    "#################--          data_preprocess        --#################\n",
    "def data_preprocess(data_frames,n_day,n_out):\n",
    "      values_df = data_frames.values    #ตัด header กับ idx ออก เป็น array matrix\n",
    "      n_features = data_frames.shape[1]\n",
    "      # print(n_features)\n",
    "      \n",
    "      # ensure all data is float\n",
    "      values = values_df.astype('float32')\n",
    "      # normalize features\n",
    "      scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "      scaled = scaler.fit_transform(values)\n",
    "\n",
    "      # frame as supervised learning\n",
    "      reframed = series_to_supervised(scaled, n_day, n_out)\n",
    "      # print(reframed.shape)\n",
    "      # print(reframed.head())\n",
    "\n",
    "      # datasets\n",
    "      values = reframed.values\n",
    "\n",
    "      #input \n",
    "      n_obs = n_day * n_features\n",
    "      dataset_X, dataset_y = values[:, :n_obs], values[:, -1]\n",
    "      # print(dataset_X.shape, len(dataset_X), dataset_y.shape) \n",
    "\n",
    "      # reshape input to be 3D [samples, timesteps, features]\n",
    "      dataset_X = dataset_X.reshape((dataset_X.shape[0], n_day, n_features))\n",
    "      # print(dataset_X.shape, dataset_y.shape)\n",
    "\n",
    "      return dataset_X,dataset_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "#################--           create_model           --#################\n",
    "def create_model(units,shape_1,shape_2,activation = 'relu'):\n",
    "      return tf.keras.models.Sequential([\n",
    "                  # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "                  keras.layers.LSTM(units=units, input_shape=(shape_1, shape_2),activation=activation),\n",
    "                  # keras.BatchNormalization(),\n",
    "                  keras.layers.Dense(units=1)\n",
    "            ])            \n",
    " \n",
    "#################--           train_model           --################# \n",
    "\n",
    "# Save checkpoints during training\n",
    "checkpoint_path = \"./Export_lstm/\" + \"last_checkpoints/\" +'lstm_ckpt'+'/'+\"cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir_new = os.path.dirname(checkpoint_path)\n",
    "# print(checkpoint_dir_new)\n",
    "\n",
    "def train_model(model,train_x, train_y,val_x, val_y,Epochs,batch_size,optimizer = 0.0001,loss_input = 'mae',metrics_input = 'accuracy'):\n",
    "      print(\"train_model......\")\n",
    "      Optimizer = tf.keras.optimizers.Adam(optimizer)\n",
    "      model.compile(Optimizer, loss=loss_input, metrics= metrics_input)\n",
    "\n",
    "      time_save = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "      logdir = os.path.join(\"logs\", time_save)\n",
    "      tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "      # Create a callback that saves the model's weights every 5 epochs\n",
    "      cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,verbose=1,save_weights_only=True,save_freq=\"epoch\",period=100)\n",
    "      \n",
    "      training_history =  model.fit(train_x, train_y, \n",
    "                  epochs=Epochs, \n",
    "                  batch_size=batch_size, \n",
    "                  validation_data=(val_x, val_y), \n",
    "                  verbose=1, \n",
    "                  # callbacks=[cp_callback,es_callback], \n",
    "                  # callbacks=[cp_callback],\n",
    "                  callbacks=[tensorboard_callback,cp_callback])\n",
    "                  # shuffle=False)\n",
    "      return training_history,model,time_save\n",
    "\n",
    "#################--           open TensorBoard          --#################   \n",
    "def displaytensor():\n",
    "      %tensorboard --logdir logs --port=6006\n",
    "      # from tensorboard import notebook\n",
    "      # notebook.list() # View open TensorBoard instances\n",
    "      # notebook.display(port=6006, height=1000) \n",
    "      import webbrowser\n",
    "      # generate an URL\n",
    "      url = 'http://localhost:6006/'\n",
    "      webbrowser.open(url)\n",
    "      \n",
    "# Function to convert  \n",
    "def listToString(s): \n",
    "    # initialize an empty string\n",
    "    str = \" \"     \n",
    "    # return string  \n",
    "    return (str.join(s))\n",
    "\n",
    "def StringTolist(string):\n",
    "    li = list(string.split(\" \"))\n",
    "    return li\n",
    "    \n",
    "#################--           Save model          --#################  \n",
    "def save_model(model_save,name_model = \"sut_rice_model_1\"):\n",
    "      print('save')\n",
    "      ## Set Name and Time ## Make folder\n",
    "      # Export_folder_name = \"./Export_lstm/model/\"\n",
    "      Export_folder_name = \"./Export_lstm/model/\"\n",
    "       \n",
    "      newfolder_name = time_out+\"_\"+name_model\n",
    "      path_newfolder = Export_folder_name\n",
    "      path_newfolder_save = os.path.join(path_newfolder, newfolder_name)\n",
    "      try: \n",
    "            os.mkdir(path_newfolder_save) \n",
    "      except OSError as error: \n",
    "            print(error)  \n",
    "\n",
    "      print(\"Directory '% s' created\" % path_newfolder_save)\n",
    "      Export_folder_name = path_newfolder_save + '/'\n",
    "\n",
    "      ## Save model\n",
    "      model_name = \"model_lstm\"\n",
    "      model_file = Export_folder_name + model_name\n",
    "      model_save.save(model_file)\n",
    "\n",
    "      # save history\n",
    "      history_name = \"hist_lstm.npy\"\n",
    "      history_file = Export_folder_name + history_name\n",
    "      np.save(history_file,history_out.history)\n",
    "      \n",
    "      ## save discription training data\n",
    "      txt_name = 'Training_model_data_discription.txt'\n",
    "      filepath_save_txt = Export_folder_name + txt_name\n",
    "      f = open(filepath_save_txt, \"a\")\n",
    "\n",
    "      f.write('Model:'+name_model)\n",
    "      f.write('\\n')\n",
    "      lines_2 = ['station:',dropdown_name_st.value]\n",
    "      f.write(''.join(lines_2))\n",
    "      f.write('\\n')\n",
    "      lines_3 = ['Year-train:',dropdown_year_train1.value+'-'+dropdown_year_train2.value]\n",
    "      f.write(''.join(lines_3))\n",
    "      f.write('\\n')\n",
    "      lines_4 = ['Year-val:',dropdown_year_val1.value+'-'+dropdown_year_val2.value]\n",
    "      f.write(''.join(lines_4))\n",
    "      f.write('\\n')\n",
    "      lines_5 = ['features-num:',str(len(selected_data))]\n",
    "      f.write(''.join(lines_5))\n",
    "      f.write('\\n')\n",
    "      lines_6 = ['features-drop:',listToString(selected_data)]\n",
    "      f.write(''.join(lines_6))\n",
    "      f.write('\\n')\n",
    "      lines_7 = ['Sampling:',dropdown_sampling_data.value]\n",
    "      f.write(''.join(lines_7))\n",
    "      f.write('\\n')\n",
    "      lines_8 = ['time-lag:',str(Box_nday.value)]  \n",
    "      f.write(''.join(lines_8))\n",
    "      f.write('\\n')\n",
    "      lines_9 = ['time-forecast:',str(Box_nout.value)]\n",
    "      f.write(''.join(lines_9))\n",
    "      f.write('\\n')\n",
    "      lines_10 = ['data-Model:','Epochs:',str(Slider_Epochs.value),'batch_size:',str(Slider_batch_size.value),'Units:',str(Box_Units.value),'activation:',dropdown_activation.value,'loss:',dropdown_loss.value]  \n",
    "      f.write(''.join(lines_10))\n",
    "      f.write('\\n')\n",
    "      lines_11 = ['performance:','loss:', str(round(history_out.history['loss'][-1],5)),'accuracy:', str(round(history_out.history['accuracy'][-1],5)),'val-loss:', str(round(history_out.history['val_loss'][-1],5)),'val-accuracy:', str(round(history_out.history['val_accuracy'][-1],5))]  \n",
    "      f.write(''.join(lines_11))\n",
    "      f.close()\n",
    "\n",
    "#################--           forecast_model          --#################  \n",
    "def forecast_model(df_predict,n_day,n_out):\n",
    "      values_df = df_predict.values    #ตัด header กับ idx ออก เป็น array matrix\n",
    "      n_features = df_predict.shape[1]\n",
    "      # print(n_features)\n",
    "\n",
    "      # ensure all data is float\n",
    "      values = values_df.astype('float32')\n",
    "      # normalize features\n",
    "      scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "      scaled = scaler.fit_transform(values)\n",
    "\n",
    "      # frame as supervised learning\n",
    "      reframed = series_to_supervised(scaled, n_day, n_out)\n",
    "      # print(reframed.shape)\n",
    "      # print(reframed.head())\n",
    "\n",
    "      # datasets\n",
    "      values = reframed.values\n",
    "\n",
    "      #input \n",
    "      n_obs = n_day * n_features\n",
    "      dataset_X, dataset_y = values[:, :n_obs], values[:, -1]\n",
    "      # print(dataset_X.shape, len(dataset_X), dataset_y.shape) \n",
    "\n",
    "      # reshape input to be 3D [samples, timesteps, features]\n",
    "      dataset_X = dataset_X.reshape((dataset_X.shape[0], n_day, n_features))\n",
    "      # print(dataset_X.shape, dataset_y.shape)\n",
    "\n",
    "\n",
    "\n",
    "      # # make a prediction\n",
    "      yhat = model_loaded.predict(dataset_X)\n",
    "      test_X_reshape = dataset_X.reshape((dataset_X.shape[0], n_day*n_features))\n",
    "      \n",
    "      # invert scaling for forecast\n",
    "      # inv_yhat = concatenate((yhat, test_X[:, -29:]), axis=1)\n",
    "      inv_yhat = concatenate((test_X_reshape[:, :(n_features-1)], yhat), axis=1)\n",
    "      inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "      inv_yhat = inv_yhat[:,-1]\n",
    "\n",
    "      # invert scaling for actual\n",
    "      test_y_reshape = dataset_y.reshape((len(dataset_y), 1))\n",
    "      # inv_y = concatenate((test_y, test_X[:, -29:]), axis=1)\n",
    "      inv_y = concatenate((test_X_reshape[:, :(n_features-1)], test_y_reshape), axis=1)\n",
    "      inv_y = scaler.inverse_transform(inv_y)\n",
    "      inv_y = inv_y[:,-1]\n",
    "      # # calculate RMSE\n",
    "      # rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "      # print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "      forecast_data = (abs(inv_yhat[:])) \n",
    "      label_data =  inv_y[:]\n",
    "\n",
    "      # calculate RMSE\n",
    "      rmse = sqrt(mean_squared_error(label_data,forecast_data))\n",
    "\n",
    "      print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "      # df = frames_test.loc[date_start:date_stop].reset_index()\n",
    "      df = frames_predict.reset_index()\n",
    "      date_time_predict = pd.to_datetime(df.pop('date'))\n",
    "\n",
    "      from matplotlib import pyplot as plt\n",
    "      def plot_Perfor():\n",
    "            plt.figure()\n",
    "            plt.plot(date_time_predict[n_day+n_out-1:],label_data,label='label',marker='.')\n",
    "            plt.plot(date_time_predict[n_day+n_out-1:],forecast_data,label='forecast',marker='.')\n",
    "\n",
    "            plt.ylabel('BPH volume')\n",
    "            plt.xlabel('Date time')\n",
    "            # plt.title(file_name +'  RMSE: %.3f' % rmse)\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            # plt.show()\n",
    "\n",
    "      plot_Perfor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c72caf8d0c4c889bf5fabe3f0791c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='station :', options=('ALL', 'Chachoengsao Rice research Ce…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9de08fe2a54e8392e47db6c40ae3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output()), _titles={'0': '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#########################################-- Dashboard Input Data--#########################################\n",
    "ALL = 'ALL'\n",
    "\n",
    "def unique_sorted_values_plus_ALL(array):\n",
    "    unique = array.unique().tolist()\n",
    "    unique.sort()\n",
    "    unique.insert(0, ALL)\n",
    "    return unique\n",
    "\n",
    "#################-- DataFrame station --#################\n",
    "output_df_st = widgets.Output()\n",
    "def dropdown_station_eventhandler(change):\n",
    "    output_df_st.clear_output()\n",
    "    with output_df_st:  \n",
    "        if (change.new == ALL):\n",
    "            display(df_st)\n",
    "        else:\n",
    "            display(df_st[df_st.nameEng == change.new])\n",
    "\n",
    "#################-- DataFrame station --#################\n",
    "output_df_st_predict = widgets.Output()\n",
    "def dropdown_station_predict_eventhandler(change):\n",
    "    output_df_st_predict.clear_output()\n",
    "    with output_df_st_predict:  \n",
    "        if (change.new == ALL):\n",
    "            display(df_st)\n",
    "        else:\n",
    "            display(df_st[df_st.nameEng == change.new])\n",
    "\n",
    "#################-- load data  --#################\n",
    "\n",
    "\n",
    "output_df_train = widgets.Output()\n",
    "output_df_val = widgets.Output()\n",
    "output_df_test = widgets.Output()\n",
    "output_df_col = widgets.Output()\n",
    "def clicked_load(b):\n",
    "    global df_out_train\n",
    "    global df_out_val\n",
    "    global df_out_test\n",
    "    global selected_data \n",
    "    output_df_train.clear_output()\n",
    "    output_df_val.clear_output()\n",
    "    output_df_test.clear_output()\n",
    "    output_df_col.clear_output()\n",
    "    with output_df_train:\n",
    "        df_out_train = creat_dataset(dropdown_name_st.value,dropdown_year_train1.value,dropdown_year_train2.value,dropdown_sampling_data.value)\n",
    "        display(df_out_train)\n",
    "        # display(creat_dataset(dropdown_name_st.value,dropdown_year_train1.value,dropdown_year_train2.value,dropdown_sampling_data.value))\n",
    "    with output_df_val:\n",
    "        df_out_val = creat_dataset(dropdown_name_st.value,dropdown_year_val1.value,dropdown_year_val2.value,dropdown_sampling_data.value)\n",
    "        display(df_out_val)\n",
    "        # display(creat_dataset(dropdown_name_st.value,dropdown_year_val1.value,dropdown_year_val2.value,dropdown_sampling_data.value))\n",
    "    with output_df_test:\n",
    "        df_out_test = creat_dataset(dropdown_name_st.value,dropdown_year_test1.value,dropdown_year_test2.value,dropdown_sampling_data.value)\n",
    "        display(df_out_test)\n",
    "        # display(creat_dataset(dropdown_name_st.value,dropdown_year_test1.value,dropdown_year_test2.value,dropdown_sampling_data.value))\n",
    "    with output_df_col:\n",
    "        checkbox_objects = []\n",
    "        # global selected_data\n",
    "        data = sorted(set(chek_list(df_out_train)),reverse=True)     \n",
    "        names = []\n",
    "        for key in data:\n",
    "                checkbox_objects.append(widgets.Checkbox(value=True, description=key))\n",
    "                names.append(key)\n",
    " \n",
    "        arg_dict = {names[i]: checkbox for i, checkbox in enumerate(checkbox_objects)}\n",
    "        ui = widgets.HBox(children=checkbox_objects)\n",
    "        display(ui)\n",
    "             \n",
    "        selected_data = []\n",
    "        def select_data(**kwargs):\n",
    "            selected_data.clear()\n",
    "            for key in kwargs:\n",
    "                if kwargs[key] is True:\n",
    "                    selected_data.append(key)\n",
    "            print(selected_data)\n",
    "\n",
    "        widgets.interactive_output(select_data, arg_dict)\n",
    "    \n",
    " \n",
    "#################-- Export data  --#################\n",
    "df_train = widgets.Output()\n",
    "df_val = widgets.Output()\n",
    "df_test = widgets.Output()\n",
    "def clicked_export(b):\n",
    "    df_train.clear_output()\n",
    "    df_val.clear_output()\n",
    "    df_test.clear_output()\n",
    "    global frames_train\n",
    "    global frames_val\n",
    "    global frames_test\n",
    "    with df_train:\n",
    "        frames_train = drop_col(df_out_train,selected_data) \n",
    "        display(frames_train)\n",
    "    with df_val:\n",
    "        frames_val = drop_col(df_out_val,selected_data) \n",
    "        display(frames_val)  \n",
    "    with df_test:\n",
    "        frames_test = drop_col(df_out_test,selected_data) \n",
    "        display(frames_test)  \n",
    "\n",
    "def chek_list(df_ch):\n",
    "        data_col = set(df_ch.columns)\n",
    "        return data_col      \n",
    "\n",
    "#################----------------------------- DashBoard display ----------------------------------#################\n",
    "dropdown_name_st = widgets.Dropdown(options = unique_sorted_values_plus_ALL(df_st.nameEng),description='station :')\n",
    "dropdown_sampling_data = widgets.Dropdown(options = ['All','3-DAY-All','7-DAY-All','14-DAY-All','3-DAY-Sampling','7-DAY-Sampling','14-DAY-Sampling'],valure = 'All-DAY',description='Sampling :')\n",
    "dropdown_year_train1 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],description='Year-train :')\n",
    "dropdown_year_train2 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2017')\n",
    "dropdown_year_val1 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2018',description='Year-val :')\n",
    "dropdown_year_val2 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2018')\n",
    "dropdown_year_test1 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2019',description='Year-test :')\n",
    "dropdown_year_test2 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2019')\n",
    "\n",
    "\n",
    "button_download = widgets.Button(description='load data',disabled=False,button_style='success',tooltip='Click me',icon='check')\n",
    "button_export = widgets.Button(description='export dataset',disabled=False,button_style='success',tooltip='Click me',icon='check')\n",
    "\n",
    "button_download.on_click(clicked_load)\n",
    "button_export.on_click(clicked_export)\n",
    "\n",
    "dropdown_name_st.observe(dropdown_station_eventhandler, names='value')\n",
    "\n",
    "\n",
    "item_layout = widgets.Layout(margin='0 0 15px 0')\n",
    "input_widgets_row1 = widgets.HBox([dropdown_name_st,dropdown_year_train1,dropdown_year_train2,dropdown_year_val1,dropdown_year_val2,dropdown_year_test1,dropdown_year_test2],layout = item_layout)\n",
    "input_widgets_row2 = widgets.HBox([dropdown_sampling_data,button_download,button_export],layout = item_layout)\n",
    "\n",
    "tab_dataset = widgets.Tab([output_df_st,output_df_train,output_df_val,output_df_test,output_df_col,df_train,df_val,df_test])\n",
    "tab_dataset.set_title(0, 'Dataset station')\n",
    "tab_dataset.set_title(1, 'train-data')\n",
    "tab_dataset.set_title(2, 'validation-data')\n",
    "tab_dataset.set_title(3, 'test-data')\n",
    "tab_dataset.set_title(4, 'Drop-data')\n",
    "tab_dataset.set_title(5, 'Dataset-train')\n",
    "tab_dataset.set_title(6, 'Dataset-validation')\n",
    "tab_dataset.set_title(7, 'Dataset-test')\n",
    "dashboard_dataset = widgets.VBox([input_widgets_row1,input_widgets_row2])\n",
    "display(dashboard_dataset)\n",
    "display(tab_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce9a51f272c433b998b0adce4b17cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=10, continuous_update=False, description='Epochs :', max=5000, m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ea41c825c1406494b30c62096c20e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output()), _titles={'0': 'model.summary', '1': 'model_fit', '2': 'model_save…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#########################################-- Dashboard train model--#########################################\n",
    "#################-- select model --#################\n",
    "# def Slider_Epochs_eventhandler(change):\n",
    "#     return Slider_Epochs\n",
    "# def Slider_batch_size_eventhandler(change):\n",
    "#     return Slider_batch_size\n",
    "# def Box_Units_eventhandler(change):\n",
    "#     return Box_Units\n",
    "# def Box_Optimizer_eventhandler(change):\n",
    "#     return Box_Optimizer\n",
    "# def dropdown_activation_eventhandler(change):\n",
    "#     return dropdown_activation\n",
    "# def dropdown_loss_eventhandler(change):\n",
    "#     return dropdown_loss\n",
    "# def Box_nday_eventhandler(change):\n",
    "#     return Box_nday\n",
    "# def Box_nout_eventhandler(change):\n",
    "#     return Box_nout\n",
    "# def string_name_save_eventhandler(change):\n",
    "#     return string_name_save\n",
    "\n",
    "# #################-- select predict --#################\n",
    "# def dropdown_start_date_eventhandler(change):\n",
    "#     return start_date\n",
    "# def dropdown_end_date_eventhandler(change):\n",
    "#     return end_date\n",
    "def dropdown_file_eventhandler(change):\n",
    "    dropdown_folder_list.options = get_list_folder()\n",
    "\n",
    "\n",
    "#################-- create_model  --#################\n",
    "output_create_model = widgets.Output()\n",
    "def clicked_model(b):\n",
    "    output_create_model.clear_output()\n",
    "    global model_in\n",
    "    global train_X\n",
    "    global train_Y\n",
    "    global val_X\n",
    "    global val_Y\n",
    "    \n",
    "    \n",
    "    with output_create_model:\n",
    "        train_X, train_Y = data_preprocess(frames_train,Box_nday.value,Box_nout.value)\n",
    "        val_X,val_Y = data_preprocess(frames_val,Box_nday.value,Box_nout.value)              \n",
    "        model_in = create_model(Box_Units.value,train_X.shape[1],train_X.shape[2],dropdown_activation.value)\n",
    "        clear_output(wait=True) \n",
    "        print(model_in.summary())\n",
    "        print(train_X.shape)  \n",
    "        print(train_Y.shape) \n",
    "        \n",
    "#################-- fit_model  --#################\n",
    "output_fit_model = widgets.Output()   \n",
    "def clicked_train(b):\n",
    "    output_fit_model.clear_output()\n",
    "    global history_out,model_out,time_out\n",
    "       \n",
    "    with output_fit_model:\n",
    "        # fit network\n",
    "        # (train_x, train_y,val_x, val_y,Epochs,batch_size,optimi = 0.0001,loss_input = 'mae',metrics_input = 'accuracy'):\n",
    "        history_out,model_out,time_out = train_model(model_in,train_X, train_Y,val_X, val_Y,Slider_Epochs.value,Slider_batch_size.value,\n",
    "                                Box_Optimizer.value,dropdown_loss.value)\n",
    "        print(\"Average test loss: \", np.average(history_out.history['loss']))\n",
    "        \n",
    "      \n",
    " #################-- tensorboard display  --#################\n",
    "output_tensorboard = widgets.Output()      \n",
    "def clicked_tensor(b):\n",
    "    output_tensorboard.clear_output()\n",
    "    with output_tensorboard:\n",
    "         displaytensor()\n",
    "\n",
    "#################--      save model     --#################\n",
    "output_save_model = widgets.Output()   \n",
    "def clicked_save(b):\n",
    "    output_save_model.clear_output()\n",
    "    \n",
    "    with output_save_model:\n",
    "        # fit network\n",
    "        filename_model = string_name_save.value\n",
    "        save_model(model_out, filename_model)\n",
    "        print(\"save-model :%s\" %(string_name_save.value))\n",
    "\n",
    "button_download = widgets.Button(description='load data',disabled=False,button_style='success',tooltip='Click me',icon='check')\n",
    "button_export = widgets.Button(description='export dataset',disabled=False,button_style='success',tooltip='Click me',icon='check')\n",
    "button_model = widgets.Button(description='create_model',disabled=False,button_style='success',tooltip='Click me',icon='check')\n",
    "button_train = widgets.Button(description='train_model',disabled=False,button_style='success',tooltip='Click me',icon='check')\n",
    "button_tensorboard = widgets.Button(description='tensorboard',disabled=False,button_style='success',tooltip='Click me',icon='check')\n",
    "button_save = widgets.Button(description='save model',disabled=False,button_style='success',tooltip='Click me',icon='check')\n",
    "\n",
    "Slider_Epochs = widgets.IntSlider(value=10,min=10,max=5000,step=10,description='Epochs :',disabled=False,continuous_update=False,orientation='horizontal', readout=True,readout_format='d')\n",
    "Slider_batch_size = widgets.IntSlider(value=128,min=16,max=256,step=8,description='Batch_size :',disabled=False,continuous_update=False,orientation='horizontal', readout=True,readout_format='d')\n",
    "Box_Units = widgets.IntText(value=50,description='units :',disabled=False)\n",
    "Box_Optimizer = widgets.FloatText(value=0.0001,step=0.0001,description='Optimizer :',disabled=False)\n",
    "dropdown_activation = widgets.Dropdown(options = ['relu','elu','sigmoid','tanh'],valure = 'elu',description='activation :')\n",
    "dropdown_loss = widgets.Dropdown(options = ['mae','mae'],valure = 'mae',description='loss :')\n",
    "Box_nday = widgets.IntText(value=7,description='n-day :',disabled=False)\n",
    "Box_nout = widgets.IntText(value=1,description='n-out :',disabled=False)\n",
    "\n",
    "start_date = widgets.DatePicker(description='Start Date',disabled=False)\n",
    "end_date = widgets.DatePicker(description='End Date',disabled=False)\n",
    "dropdown_folder_list = widgets.Dropdown(options = get_list_folder())\n",
    "\n",
    "string_name_save = widgets.Text(value='sut_rice_model_0',placeholder='Type something',description='name model:',disabled=False)\n",
    "\n",
    "button_model.on_click(clicked_model)\n",
    "button_train.on_click(clicked_train)\n",
    "button_tensorboard.on_click(clicked_tensor)\n",
    "button_save.on_click(clicked_save)\n",
    "\n",
    "# Slider_Epochs.observe(Slider_Epochs_eventhandler, names='value')\n",
    "# Slider_batch_size.observe(Slider_batch_size_eventhandler, names='value')\n",
    "# Box_Units.observe(Box_Units_eventhandler, names='value')\n",
    "# Box_Optimizer.observe(Box_Optimizer_eventhandler, names='value')\n",
    "# dropdown_activation.observe(dropdown_activation_eventhandler, names='value')\n",
    "# dropdown_loss.observe(dropdown_loss_eventhandler, names='value')\n",
    "# Box_nday.observe(Box_nday_eventhandler, names='value')\n",
    "# Box_nout.observe(Box_nout_eventhandler, names='value')\n",
    "# start_date.observe(dropdown_start_date_eventhandler, names='values')\n",
    "# end_date.observe(dropdown_end_date_eventhandler, names='values')\n",
    "dropdown_folder_list.observe(dropdown_file_eventhandler)\n",
    "# string_name_save.observe(string_name_save_eventhandler, names='values')\n",
    "\n",
    "input_widgets_Model_row1 = widgets.HBox([Slider_Epochs,Slider_batch_size,Box_Units,Box_Optimizer],layout = item_layout)\n",
    "input_widgets_Model_row2 = widgets.HBox([dropdown_activation,dropdown_loss,Box_nday,Box_nout],layout = item_layout)\n",
    "input_widgets_Model_row3 = widgets.HBox([button_model,button_train,button_tensorboard,string_name_save,button_save])\n",
    "\n",
    "dashboard_model = widgets.VBox([input_widgets_Model_row1,input_widgets_Model_row2,input_widgets_Model_row3])\n",
    "tab_model = widgets.Tab([output_create_model,output_fit_model,output_save_model])\n",
    "tab_model.set_title(0,'model.summary')\n",
    "tab_model.set_title(1,'model_fit')\n",
    "tab_model.set_title(2,'model_save')\n",
    "display(dashboard_model)\n",
    "display(tab_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d128a68a094122b1a60c690e7cf5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='station :', options=('ALL', 'Chachoengsao Rice research Ce…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec64ab349b94c87ac21e20cc8642242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output(), Output(), Output()), _titles={'0': 'Dataset station', '1': 'Load M…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#########################################-- Dashboard prediction--#########################################\n",
    "\n",
    "#################-- DataFrame station --#################\n",
    "output_df_st_predict = widgets.Output()\n",
    "def dropdown_station_predict_eventhandler(change):\n",
    "    output_df_st_predict.clear_output()\n",
    "    with output_df_st_predict:  \n",
    "        if (change.new == ALL):\n",
    "            display(df_st)\n",
    "        else:\n",
    "            display(df_st[df_st.nameEng == change.new])\n",
    "\n",
    "#################-- select predict --#################\n",
    "def dropdown_start_date_eventhandler(change):\n",
    "    return start_date\n",
    "def dropdown_end_date_eventhandler(change):\n",
    "    return end_date\n",
    "def dropdown_file_eventhandler(change):\n",
    "    dropdown_folder_list.options = get_list_folder()\n",
    "\n",
    "#################-- load data predict  --#################\n",
    "output_df_predict = widgets.Output()\n",
    "output_description_data = widgets.Output()\n",
    "\n",
    "def clicked_load_predict(b):   \n",
    "    global frames_predict   \n",
    "    output_df_predict.clear_output()\n",
    "    output_description_data.clear_output()\n",
    "    with output_description_data:\n",
    "        li  = get_file_model(dropdown_folder_list.value)\n",
    "        line_0_model_name = (li[0].split(\":\"))[1]\n",
    "        print(line_0_model_name)\n",
    "        line_4_drop = (li[5].split(\":\"))[1]\n",
    "        print(line_4_drop)\n",
    "        line_5_sampling = (li[6].split(\":\"))[1]\n",
    "        print(line_5_sampling)\n",
    "        line_6_time_lag = (li[7].split(\":\"))[1]\n",
    "        print(line_6_time_lag)\n",
    "        line_7_time_forecast = (li[8].split(\":\"))[1]\n",
    "        print(line_7_time_forecast)\n",
    "        date_start = str(start_date.value)\n",
    "        date_stop = str(end_date.value)\n",
    "        print(date_start)\n",
    "        print(date_stop)\n",
    "    with output_df_predict:\n",
    "        df_out_predict = creat_dataset(dropdown_name_st_predict.value,str(start_date.value)[0:4],str(end_date.value)[0:4],line_5_sampling)\n",
    "        frames_predict = drop_col(df_out_predict,list(line_4_drop.split(\" \"))).loc[date_start:date_stop] \n",
    "        display(frames_predict.head())\n",
    "    \n",
    "\n",
    "#################-- load_model  --#################\n",
    "output_load_model = widgets.Output()\n",
    "def clicked_load_model(b):\n",
    "    output_load_model.clear_output()\n",
    "    global model_loaded\n",
    "       \n",
    "    with output_load_model:\n",
    "        Export_folder_name = \"/Export_lstm/model/\"\n",
    "        model_name = cwd + Export_folder_name+dropdown_folder_list.value+\"/model_lstm/\"\n",
    "        print(model_name)\n",
    "        # Load model\n",
    "        model_loaded = tf.keras.models.load_model(model_name)\n",
    "        model_loaded.summary()\n",
    "\n",
    "#################-- forecast_model  --#################\n",
    "output_forecast_model = widgets.Output()\n",
    "def clicked_predict_model(b):\n",
    "    output_forecast_model.clear_output()\n",
    "    with output_forecast_model:        \n",
    "        li  = get_file_model(dropdown_folder_list.value)\n",
    "        line_6_time_lag = (li[7].split(\":\"))[1]\n",
    "        line_7_time_forecast = (li[8].split(\":\"))[1]\n",
    "\n",
    "        n_day = int(line_6_time_lag)\n",
    "        n_out = int(line_7_time_forecast)\n",
    "\n",
    "        forecast_model(frames_predict,n_day,n_out)\n",
    "\n",
    "start_date = widgets.DatePicker(description='Start Date',disabled=False)\n",
    "end_date = widgets.DatePicker(description='End Date',disabled=False)\n",
    "dropdown_folder_list = widgets.Dropdown(options = get_list_folder())\n",
    "dropdown_name_st_predict = widgets.Dropdown(options = unique_sorted_values_plus_ALL(df_st.nameEng),description='station :')\n",
    "button_load_model = widgets.Button(description='load model',disabled=False,button_style='success',tooltip='Click me',icon='check')\n",
    "button_load_data = widgets.Button(description='load data',disabled=False,button_style='success',tooltip='Click me',icon='check')\n",
    "button_predict = widgets.Button(description='Prediction',disabled=False,button_style='success',tooltip='Click me',icon='check')\n",
    "\n",
    "button_load_model.on_click(clicked_load_model)\n",
    "button_load_data.on_click(clicked_load_predict)\n",
    "button_predict.on_click(clicked_predict_model)\n",
    "\n",
    "dropdown_name_st_predict.observe(dropdown_station_predict_eventhandler, names='value')\n",
    "start_date.observe(dropdown_start_date_eventhandler, names='values')\n",
    "end_date.observe(dropdown_end_date_eventhandler, names='values')\n",
    "dropdown_folder_list.observe(dropdown_file_eventhandler)\n",
    "\n",
    "\n",
    "\n",
    "input_widgets_predict_row1 = widgets.HBox([dropdown_name_st_predict, start_date, end_date,button_load_data])\n",
    "input_widgets_predict_row2 = widgets.HBox([dropdown_folder_list,button_load_model,button_predict])\n",
    "\n",
    "dashboard_predict = widgets.VBox([input_widgets_predict_row1,input_widgets_predict_row2])\n",
    "tab_predict = widgets.Tab([output_df_st_predict,output_load_model,output_description_data,output_df_predict,output_forecast_model])\n",
    "tab_predict.set_title(0,'Dataset station')\n",
    "tab_predict.set_title(1,'Load Model')\n",
    "tab_predict.set_title(2,'description data')\n",
    "tab_predict.set_title(3,'Dataset_predict')\n",
    "tab_predict.set_title(4,'forecast')\n",
    "display(dashboard_predict)\n",
    "display(tab_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('tf_2.4_ts')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bcdeb12b9765c5f0be135c78098b34d39651f762d518870df951bed52914d1c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
