{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import IPython\n",
    "import IPython.display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import concat\n",
    "from pandas import DataFrame\n",
    "\n",
    "import datetime,os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name and Path\n",
    "cwd = os.getcwd()\n",
    "path_adress = \"/Import_Dataset/\"\n",
    "\n",
    "# ข้อมูล พิกัดสถานที่เเละรายชื้ออ้างอิงสถานที่ตรวจวัด\n",
    "file_name_st = 'Data_lat_long_Rice research Center'\n",
    "csv_file_st = cwd + path_adress + file_name_st + '.csv'\n",
    "\n",
    "df_st = pd.read_csv(csv_file_st)\n",
    "print(\"All low RiceCenter {} station\" .format(df_st.shape[0]))\n",
    "\n",
    "data_list_st_num = list(range(0, len(df_st)))\n",
    "data_list_st_name = []\n",
    "data_list_st_name = df_st['nameEng'].values.tolist()\n",
    "data_list_st_lat = df_st['Latitude'].values.tolist()\n",
    "data_list_st_long = df_st['Longitude'].values.tolist()\n",
    "\n",
    "#################-- Moving Averag data --#################\n",
    "def mavr_dataset(frames_sma,rolling_num,our_rate,mode):    \n",
    "    h_name = list(frames_sma)\n",
    "    if mode == 'sum':\n",
    "        frames_sma['mirid bug'] = frames_sma['mirid bug'].rolling(rolling_num, min_periods=1).sum().round(1)\n",
    "        frames_sma['bph'] = frames_sma['bph'].rolling(rolling_num, min_periods=1).sum().round(1)\n",
    "        frames_sma['precip'] = frames_sma['precip'].rolling(rolling_num, min_periods=1).sum().round(1)\n",
    "    else:\n",
    "        frames_sma['mirid bug'] = frames_sma['mirid bug'].rolling(rolling_num, min_periods=1).mean().round(1)\n",
    "        frames_sma['bph'] = frames_sma['bph'].rolling(rolling_num, min_periods=1).mean().round(1)\n",
    "        frames_sma['precip'] = frames_sma['precip'].rolling(rolling_num, min_periods=1).mean().round(1)\n",
    "   \n",
    "    for i in range(7,12):\n",
    "        frames_sma[h_name[i]] = frames_sma[h_name[i]].rolling(rolling_num, min_periods=1).mean().round(1)\n",
    "\n",
    "    constant_subset = frames_sma[::our_rate] \n",
    "    return constant_subset\n",
    "\n",
    "#################-- creat_dataset  --#################\n",
    "def creat_dataset(st_BPH = 'ALL', y_1='2015', y_2='2019', m_avr='ALL'):\n",
    "    for i in range(len(data_list_st_num)):\n",
    "        st = i\n",
    "        file_name = df_st['nameEng'][data_list_st_num[st]]\n",
    "        name_input = file_name\n",
    "        name_locals = 'stN_' + file_name\n",
    "\n",
    "        # File name and Path\n",
    "        csv_file = cwd + path_adress + name_input + '.csv'\n",
    "        # index_col=0 , index_col=None\n",
    "        dataset = pd.read_csv(csv_file, header=0,\n",
    "                              index_col=0, encoding=\"TIS-620\")\n",
    "\n",
    "        if m_avr == '3-DAY-All-':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,3,1,'mean')\n",
    "        elif m_avr == '7-DAY-All':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,7,1,'mean')\n",
    "        elif m_avr == '14-DAY-All':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,14,1,'mean')\n",
    "        elif m_avr == '3-DAY-Sampling':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,3,3,'sum')\n",
    "        elif m_avr == '7-DAY-Sampling':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,7,7,'sum')\n",
    "        elif m_avr == '14-DAY-Sampling':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,14,14,'sum')\n",
    "        else :\n",
    "            locals()[name_locals] = dataset\n",
    "            # print(f'Dataframe name_station: {st+1 , name_input}')\n",
    "        print('wait......')\n",
    "        del dataset\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    date_start = y_1 + '-01' + '-01'\n",
    "    date_stop = y_2 + '-12' + '-31'\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "    if st_BPH != 'ALL' :\n",
    "        # file_name = df_st['nameEng'][data_list_st_num[st_BPH]]\n",
    "        file_name = st_BPH\n",
    "        locals_input = 'stN_' + file_name\n",
    "        print(locals_input)\n",
    "        dataset_st = locals()[locals_input].loc[date_start:date_stop]\n",
    "        frames_st = dataset_st\n",
    "    else:\n",
    "        m = 0\n",
    "        for j in range(len(data_list_st_num)):\n",
    "            # for j in range(df_st.shape[0]):\n",
    "            # file_name = df_st['nameEng'][j]\n",
    "            file_name = df_st['nameEng'][data_list_st_num[j]]\n",
    "            locals_input = 'stN_' + file_name\n",
    "            print(locals_input)\n",
    "            dataset_st = locals()[locals_input].loc[date_start:date_stop]\n",
    "            clear_output(wait=True)\n",
    "            if m == 0:\n",
    "                frames_st = dataset_st\n",
    "                m = m+1\n",
    "                print(m)\n",
    "            else:\n",
    "                frames_st = [frames_st, dataset_st]\n",
    "                frames_st = pd.concat(frames_st)\n",
    "\n",
    "    # dataset    \n",
    "    frames = frames_st\n",
    "    return frames\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#################-- Drop Colum  --#################\n",
    "def drop_col(df_drop,drop = ['address']):\n",
    "    for j in range(len(drop)):\n",
    "        df_drop = df_drop.drop(drop[j], axis=1)\n",
    "    return df_drop\n",
    "\n",
    "#################--      Plot        --#################\n",
    "def plot_data(frames_train,plot_cols,df_name,start='2015',end='2017',st = 'ALL'):\n",
    "    #ตรวจสอบข้อมูล dataset \n",
    "    if st == 'ALL':\n",
    "        df_plot = frames_train[plot_cols]\n",
    "    else:\n",
    "        df_plot = frames_train[plot_cols].loc[start:end]\n",
    "    \n",
    "    plt.figure()    \n",
    "    # df_plot.plot(lw=1,grid=True,figsize=(13,30),subplots=True)\n",
    "    # df_plot.plot(lw=1,grid=True,subplots=True)\n",
    "    # df_plot.plot(marker='.',grid=True,linestyle = 'solid',subplots=True)\n",
    "    df_plot.plot(marker='.',grid=True,linestyle = 'solid')\n",
    "    plt.xlabel('Date time-'+ df_name)\n",
    "    plt.legend()\n",
    "    # plt.show()   \n",
    "\n",
    "#################--   get list of folders in directory   --#################\n",
    "dir_path = cwd + \"/Export_lstm/model/\"\n",
    "def get_list_folder():\n",
    "    folder_list = os.listdir(dir_path)\n",
    "    return folder_list\n",
    "\n",
    "def get_file_model(file_name):\n",
    "    txt = dir_path+file_name+\"/Training_model_data_discription.txt\"\n",
    "    string_data = open(txt,\"r\").read()\n",
    "    list_txt = list(string_data.split(\"\\n\"))\n",
    "    return list_txt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################-- Dashboard Input Data--#########################################\n",
    "ALL = 'ALL'\n",
    "\n",
    "def unique_sorted_values_plus_ALL(array):\n",
    "    unique = array.unique().tolist()\n",
    "    unique.sort()\n",
    "    unique.insert(0, ALL)\n",
    "    return unique\n",
    "\n",
    "#################-- DataFrame station --#################\n",
    "output_df_st = widgets.Output()\n",
    "def dropdown_station_eventhandler(change):\n",
    "    output_df_st.clear_output()\n",
    "    with output_df_st:  \n",
    "        if (change.new == ALL):\n",
    "            display(df_st)\n",
    "        else:\n",
    "            display(df_st[df_st.nameEng == change.new])\n",
    "\n",
    "#################-- DataFrame station --#################\n",
    "output_df_st_predict = widgets.Output()\n",
    "def dropdown_station_predict_eventhandler(change):\n",
    "    output_df_st_predict.clear_output()\n",
    "    with output_df_st_predict:  \n",
    "        if (change.new == ALL):\n",
    "            display(df_st)\n",
    "        else:\n",
    "            display(df_st[df_st.nameEng == change.new])\n",
    "\n",
    "#################-- load data  --#################\n",
    "\n",
    "\n",
    "output_df_train = widgets.Output()\n",
    "output_df_val = widgets.Output()\n",
    "output_df_test = widgets.Output()\n",
    "output_df_col = widgets.Output()\n",
    "def clicked_load(b):\n",
    "    global df_out_train\n",
    "    global df_out_val\n",
    "    global df_out_test\n",
    "    global selected_data \n",
    "    output_df_train.clear_output()\n",
    "    output_df_val.clear_output()\n",
    "    output_df_test.clear_output()\n",
    "    output_df_col.clear_output()\n",
    "    with output_df_train:\n",
    "        df_out_train = creat_dataset(dropdown_name_st.value,dropdown_year_train1.value,dropdown_year_train2.value,dropdown_sampling_data.value)\n",
    "        display(df_out_train)\n",
    "        # display(creat_dataset(dropdown_name_st.value,dropdown_year_train1.value,dropdown_year_train2.value,dropdown_sampling_data.value))\n",
    "    with output_df_val:\n",
    "        df_out_val = creat_dataset(dropdown_name_st.value,dropdown_year_val1.value,dropdown_year_val2.value,dropdown_sampling_data.value)\n",
    "        display(df_out_val)\n",
    "        # display(creat_dataset(dropdown_name_st.value,dropdown_year_val1.value,dropdown_year_val2.value,dropdown_sampling_data.value))\n",
    "    with output_df_test:\n",
    "        df_out_test = creat_dataset(dropdown_name_st.value,dropdown_year_test1.value,dropdown_year_test2.value,dropdown_sampling_data.value)\n",
    "        display(df_out_test)\n",
    "        # display(creat_dataset(dropdown_name_st.value,dropdown_year_test1.value,dropdown_year_test2.value,dropdown_sampling_data.value))\n",
    "    with output_df_col:\n",
    "        checkbox_objects = []\n",
    "        # global selected_data\n",
    "        data = sorted(set(chek_list(df_out_train)),reverse=True)     \n",
    "        names = []\n",
    "        for key in data:\n",
    "                checkbox_objects.append(widgets.Checkbox(value=True, description=key))\n",
    "                names.append(key)\n",
    " \n",
    "        arg_dict = {names[i]: checkbox for i, checkbox in enumerate(checkbox_objects)}\n",
    "        ui = widgets.HBox(children=checkbox_objects)\n",
    "        display(ui)\n",
    "             \n",
    "        selected_data = []\n",
    "        def select_data(**kwargs):\n",
    "            selected_data.clear()\n",
    "            for key in kwargs:\n",
    "                if kwargs[key] is True:\n",
    "                    selected_data.append(key)\n",
    "            print(selected_data)\n",
    "\n",
    "        widgets.interactive_output(select_data, arg_dict)\n",
    "    \n",
    " \n",
    "#################-- Export data  --#################\n",
    "df_train = widgets.Output()\n",
    "df_val = widgets.Output()\n",
    "df_test = widgets.Output()\n",
    "def clicked_export(b):\n",
    "    df_train.clear_output()\n",
    "    df_val.clear_output()\n",
    "    df_test.clear_output()\n",
    "    global frames_train\n",
    "    global frames_val\n",
    "    global frames_test\n",
    "    with df_train:\n",
    "        frames_train = drop_col(df_out_train,selected_data) \n",
    "        display(frames_train)\n",
    "    with df_val:\n",
    "        frames_val = drop_col(df_out_val,selected_data) \n",
    "        display(frames_val)  \n",
    "    with df_test:\n",
    "        frames_test = drop_col(df_out_test,selected_data) \n",
    "        display(frames_test)  \n",
    "\n",
    "def chek_list(df_ch):\n",
    "        data_col = set(df_ch.columns)\n",
    "        return data_col      \n",
    "\n",
    "#################----------------------------- DashBoard display ----------------------------------#################\n",
    "dropdown_name_st = widgets.Dropdown(options = unique_sorted_values_plus_ALL(df_st.nameEng),description='station :')\n",
    "dropdown_sampling_data = widgets.Dropdown(options = ['All','3-DAY-All','7-DAY-All','14-DAY-All','3-DAY-Sampling','7-DAY-Sampling','14-DAY-Sampling'],valure = 'All-DAY',description='Sampling :')\n",
    "dropdown_year_train1 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],description='Year-train :')\n",
    "dropdown_year_train2 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2017')\n",
    "dropdown_year_val1 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2018',description='Year-val :')\n",
    "dropdown_year_val2 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2018')\n",
    "dropdown_year_test1 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2019',description='Year-test :')\n",
    "dropdown_year_test2 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2019')\n",
    "\n",
    "\n",
    "button_download = widgets.Button(description='load data',disabled=False,button_style='success',tooltip='Click me',icon='check')\n",
    "button_export = widgets.Button(description='export dataset',disabled=False,button_style='success',tooltip='Click me',icon='check')\n",
    "\n",
    "button_download.on_click(clicked_load)\n",
    "button_export.on_click(clicked_export)\n",
    "\n",
    "dropdown_name_st.observe(dropdown_station_eventhandler, names='value')\n",
    "\n",
    "\n",
    "item_layout = widgets.Layout(margin='0 0 15px 0')\n",
    "input_widgets_row1 = widgets.HBox([dropdown_name_st,dropdown_year_train1,dropdown_year_train2,dropdown_year_val1,dropdown_year_val2,dropdown_year_test1,dropdown_year_test2],layout = item_layout)\n",
    "input_widgets_row2 = widgets.HBox([dropdown_sampling_data,button_download,button_export],layout = item_layout)\n",
    "\n",
    "tab_dataset = widgets.Tab([output_df_st,output_df_train,output_df_val,output_df_test,output_df_col,df_train,df_val,df_test])\n",
    "tab_dataset.set_title(0, 'Dataset station')\n",
    "tab_dataset.set_title(1, 'train-data')\n",
    "tab_dataset.set_title(2, 'validation-data')\n",
    "tab_dataset.set_title(3, 'test-data')\n",
    "tab_dataset.set_title(4, 'Drop-data')\n",
    "tab_dataset.set_title(5, 'Dataset-train')\n",
    "tab_dataset.set_title(6, 'Dataset-validation')\n",
    "tab_dataset.set_title(7, 'Dataset-test')\n",
    "dashboard_dataset = widgets.VBox([input_widgets_row1,input_widgets_row2])\n",
    "display(dashboard_dataset)\n",
    "display(tab_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "def plot_data(frames_train,df_name):\n",
    "    #ตรวจสอบข้อมูล dataset \n",
    "    plt.figure()\n",
    "    df_plot = frames_train\n",
    "    df_plot.plot(lw=1,grid=True,figsize=(15,7),subplots=True)\n",
    "    plt.xlabel('time (day) - '+ df_name)\n",
    "    plt.legend()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_month = frames_train['day'].values\n",
    "month_of_year = frames_train['month'].values\n",
    "\n",
    "frames_train['d_sin'] = np.sin((2*np.pi)/30*frames_train['day'].values)\n",
    "frames_train['d_cos'] = np.cos((2*np.pi)/30*frames_train['day'].values)\n",
    "frames_train['m_sin'] = np.sin((2*np.pi)/12*frames_train['month'].values)\n",
    "frames_train['m_cos'] = np.cos((2*np.pi)/12*frames_train['month'].values)\n",
    "frames_train = frames_train.drop(['day', 'month'], axis=1)\n",
    "\n",
    "frames_val['d_sin'] = np.sin((2*np.pi)/30*frames_val['day'].values)\n",
    "frames_val['d_cos'] = np.cos((2*np.pi)/30*frames_val['day'].values)\n",
    "frames_val['m_sin'] = np.sin((2*np.pi)/12*frames_val['month'].values)\n",
    "frames_val['m_cos'] = np.cos((2*np.pi)/12*frames_val['month'].values)\n",
    "frames_val = frames_val.drop(['day', 'month'], axis=1)\n",
    "\n",
    "\n",
    "frames_test['d_sin'] = np.sin((2*np.pi)/30*frames_test['day'].values)\n",
    "frames_test['d_cos'] = np.cos((2*np.pi)/30*frames_test['day'].values)\n",
    "frames_test['m_sin'] = np.sin((2*np.pi)/12*frames_test['month'].values)\n",
    "frames_test['m_cos'] = np.cos((2*np.pi)/12*frames_test['month'].values)\n",
    "frames_test = frames_test.drop(['day', 'month'], axis=1)\n",
    "\n",
    "#define function to swap columns\n",
    "def swap_columns(df, col1, col2):\n",
    "    col_list = list(df.columns)\n",
    "    x, y = col_list.index(col1), col_list.index(col2)\n",
    "    col_list[y], col_list[x] = col_list[x], col_list[y]\n",
    "    df = df[col_list]\n",
    "    return df\n",
    "\n",
    "#swap points and rebounds columns\n",
    "frames_train = swap_columns(frames_train, 'bph', list(frames_train.columns)[-1])\n",
    "frames_val = swap_columns(frames_val, 'bph', list(frames_val.columns)[-1])\n",
    "frames_test = swap_columns(frames_test, 'bph', list(frames_test.columns)[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multivariate = [temp,precip,humidity,bph] 4\n",
    "#Multivariate = [temp,precip,humidity,day,month,bph] 6\n",
    "#Multivariate = [temp,precip,humidity,day,month,bph] + [rice] \n",
    "\n",
    "values_train = frames_train.values    \n",
    "n_features = frames_train.shape[1]\n",
    "values_validation = frames_val.values\n",
    "plot_data(frames_train,'Data train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# In_day   Out_day  n_seq   n_steps\n",
    "#    7      1\n",
    "#    7      3\n",
    "#    7      5\n",
    "#    7      7\n",
    "#   14      1\n",
    "#   14      3\n",
    "#   14      5\n",
    "#   14      7\n",
    "#   21      1\n",
    "#   21      3\n",
    "#   21      5\n",
    "#   21      7\n",
    "\n",
    "n_day = 14\n",
    "n_out = 3\n",
    "\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(values_train, n_day, n_out)\n",
    "# print(reframed.shape)\n",
    "# print(reframed.head())\n",
    "\n",
    "# predict datasets\n",
    "values = reframed.values\n",
    "train = values\n",
    "\n",
    "#input \n",
    "n_obs = n_day * n_features\n",
    "train_X, train_y = train[:, :n_obs], train[:, -1]\n",
    "print(train_X.shape, train_y.shape)  \n",
    "\n",
    "# for i in range(len(train_X)):\n",
    "# \tprint(train_X[i], train_y[i])\n",
    "\n",
    "subsequences = 2\n",
    "timesteps = train_X.shape[1]//subsequences\n",
    "train_X = train_X.reshape((train_X.shape[0], subsequences, timesteps, 1))\n",
    "print(train_X.shape, train_y.shape)\n",
    "\n",
    "#---------------------------------------------------------------------------------#\n",
    "\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(values_validation, n_day, n_out)\n",
    "# print(reframed.shape)\n",
    "# print(reframed.head())\n",
    "\n",
    "# predict datasets\n",
    "values = reframed.values\n",
    "validat = values\n",
    "\n",
    "#input \n",
    "n_obs = n_day * n_features\n",
    "val_X, val_y = validat[:, :n_obs], validat[:, -1]\n",
    "print(val_X.shape, val_y.shape)  \n",
    "\n",
    "# for i in range(len(train_X)):\n",
    "# \tprint(val_X[i], val_y[i])\n",
    "\n",
    "timesteps = val_X.shape[1]//subsequences\n",
    "val_X = val_X.reshape((val_X.shape[0], subsequences, timesteps, 1))\n",
    "print(val_X.shape, val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs = 200\n",
    "# # define model A\n",
    "model = models.Sequential()\n",
    "model.add(layers.TimeDistributed(layers.Conv1D(filters = 64, kernel_size=1, activation='relu'), input_shape=(None, train_X.shape[2], train_X.shape[3])))\n",
    "model.add(layers.TimeDistributed(layers.Conv1D(filters = 64, kernel_size=1, activation='relu')))\n",
    "model.add(layers.TimeDistributed(layers.MaxPooling1D(pool_size=2)))\n",
    "model.add(layers.TimeDistributed(layers.Flatten()))\n",
    "model.add(layers.LSTM(50, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "# define model A\n",
    "# model = models.Sequential()\n",
    "# model.add(layers.TimeDistributed(layers.Conv1D(filters = 64, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features_model)))\n",
    "# # model.add(layers.TimeDistributed(layers.Conv1D(filters = 64, kernel_size=2, activation='relu')))\n",
    "# model.add(layers.TimeDistributed(layers.MaxPooling1D(pool_size=2)))\n",
    "# model.add(layers.TimeDistributed(layers.Flatten()))\n",
    "# model.add(layers.LSTM(100, activation='relu'))\n",
    "# model.add(layers.Dense(1))\n",
    "\n",
    "# # define model\n",
    "# model = models.Sequential()\n",
    "# model.add(layers.Conv1D(filters = 64, kernel_size=2, activation='relu', input_shape=(None, n_steps, n_features_model)))\n",
    "# # model.add(layers.Conv1D(filters = 64, kernel_size=2, activation='relu'))\n",
    "# model.add(layers.MaxPooling1D(pool_size=2))\n",
    "# model.add(layers.Flatten())\n",
    "# model.add(layers.LSTM(50, activation='relu'))\n",
    "# model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "history = model.fit(train_X, train_y, validation_data=(val_X, val_y),epochs= Epochs, verbose=2)\n",
    "\n",
    "# plot history\n",
    "from matplotlib import pyplot as plt\n",
    "def plot_loss(history):\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.xlabel('Epoch  '+'loss = %f'%history.history['loss'][-1])\n",
    "    plt.ylabel('Error')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "#     plt.show()\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_predict = frames_test.values\n",
    "n_features = frames_test.shape[1]\n",
    "df = frames_test.reset_index()\n",
    "date_time_predict = pd.to_datetime(df.pop('date'))\n",
    "\n",
    "   \n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(values_predict, n_day, n_out)\n",
    "# print(reframed.shape)\n",
    "# print(reframed.head())\n",
    "    \n",
    "# predict datasets\n",
    "values = reframed.values\n",
    "# n_train_day = int(values.shape[0]*0.9)\n",
    "# train = values[:n_train_day, :]\n",
    "test = values\n",
    "    \n",
    "# predict into input and outputs\n",
    "n_obs = n_day * n_features\n",
    "# test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "test_X, test_y = test[:, :n_obs], test[:, -1]\n",
    "print(test_X.shape, test_y.shape)\n",
    "\n",
    "timesteps = test_X.shape[1]//subsequences\n",
    "test_X = test_X.reshape((test_X.shape[0], subsequences, timesteps, 1))\n",
    "print(test_X.shape, test_y.shape)\n",
    "\n",
    "## make a prediction\n",
    "yhat = model.predict(test_X,verbose=0)\n",
    "\n",
    "df_out = pd.DataFrame(list(test_y),columns=['actual'])\n",
    "df_out['predict'] = pd.DataFrame(list(yhat))\n",
    "\n",
    "# calculate RMSE\n",
    "m = tf.keras.metrics.RootMeanSquaredError()\n",
    "m.update_state([df_out['actual']], [df_out['predict']])\n",
    "rmse = m.result().numpy()\n",
    "print('RMSE: %.3f' % rmse)\n",
    "\n",
    "\n",
    "d_range = 345\n",
    "pre_start = 0 \n",
    "\n",
    "pre_stop = pre_start+d_range\n",
    "d_start = n_day+n_out-1+pre_start\n",
    "d_stop = n_day+n_out-1+pre_start+d_range\n",
    "\n",
    "# calculate RMSE\n",
    "m = tf.keras.metrics.RootMeanSquaredError()\n",
    "m.update_state(df_out['actual'][pre_start:pre_stop], df_out['predict'][pre_start:pre_stop])\n",
    "rmse = m.result().numpy()\n",
    "print('RMSE: %.3f' % rmse)\n",
    "\n",
    "strings = list(frames_test.columns)\n",
    "textstr = '\\n'.join(strings[:12])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "ax.plot(date_time_predict[d_start:d_stop],df_out['actual'][pre_start:pre_stop],label='actual')\n",
    "ax.plot(date_time_predict[d_start:d_stop],df_out['predict'][pre_start:pre_stop],label='predict')\n",
    "\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_title('Multivariate-CNN_LSTM model-A' +'  RMSE: %.2f' % rmse)\n",
    "ax.set_xlabel('time (day) '+'(%d-'%n_day+'%d-'%n_out+'Epoch %d)'%Epochs)\n",
    "ax.set_ylabel('BPH Volume')\n",
    "ax.grid(True)\n",
    "\n",
    "# these are matplotlib.patch.Patch properties\n",
    "props = dict(boxstyle='round', alpha=0.5)\n",
    "\n",
    "# place a text box in upper left in axes coords\n",
    "ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "d_range = 60\n",
    "pre_start =  200\n",
    "\n",
    "pre_stop = pre_start+d_range\n",
    "d_start = n_day+n_out-1+pre_start\n",
    "d_stop = n_day+n_out-1+pre_start+d_range\n",
    "\n",
    "# calculate RMSE\n",
    "m = tf.keras.metrics.RootMeanSquaredError()\n",
    "m.update_state(df_out['actual'][pre_start:pre_stop], df_out['predict'][pre_start:pre_stop])\n",
    "rmse = m.result().numpy()\n",
    "print('RMSE: %.3f' % rmse)\n",
    "\n",
    "strings = list(frames_test.columns)\n",
    "textstr = '\\n'.join(strings[:12])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "ax.plot(date_time_predict[d_start:d_stop],df_out['actual'][pre_start:pre_stop],label='actual')\n",
    "ax.plot(date_time_predict[d_start:d_stop],df_out['predict'][pre_start:pre_stop],label='predict')\n",
    "\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_title('Multivariate-CNN_LSTM model-A' +'  RMSE: %.2f' % rmse)\n",
    "ax.set_xlabel('time (day) '+'(%d-'%n_day+'%d-'%n_out+'Epoch %d)'%Epochs)\n",
    "ax.set_ylabel('BPH Volume')\n",
    "ax.grid(True)\n",
    "\n",
    "# these are matplotlib.patch.Patch properties\n",
    "props = dict(boxstyle='round', alpha=0.5)\n",
    "\n",
    "# place a text box in upper left in axes coords\n",
    "ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('tf_2.4_ts')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bcdeb12b9765c5f0be135c78098b34d39651f762d518870df951bed52914d1c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
