{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import IPython\n",
    "import IPython.display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import concat\n",
    "from pandas import DataFrame\n",
    "\n",
    "import datetime,os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All low RiceCenter 34 station\n"
     ]
    }
   ],
   "source": [
    "# File name and Path\n",
    "cwd = os.getcwd()\n",
    "path_adress = \"/Import_Dataset/\"\n",
    "\n",
    "# ข้อมูล พิกัดสถานที่เเละรายชื้ออ้างอิงสถานที่ตรวจวัด\n",
    "file_name_st = 'Data_lat_long_Rice research Center'\n",
    "csv_file_st = cwd + path_adress + file_name_st + '.csv'\n",
    "\n",
    "df_st = pd.read_csv(csv_file_st)\n",
    "print(\"All low RiceCenter {} station\" .format(df_st.shape[0]))\n",
    "\n",
    "data_list_st_num = list(range(0, len(df_st)))\n",
    "data_list_st_name = []\n",
    "data_list_st_name = df_st['nameEng'].values.tolist()\n",
    "data_list_st_lat = df_st['Latitude'].values.tolist()\n",
    "data_list_st_long = df_st['Longitude'].values.tolist()\n",
    "\n",
    "#################-- Moving Averag data --#################\n",
    "def mavr_dataset(frames_sma,rolling_num,our_rate,mode):    \n",
    "    h_name = list(frames_sma)\n",
    "    if mode == 'sum':\n",
    "        frames_sma['mirid bug'] = frames_sma['mirid bug'].rolling(rolling_num, min_periods=1).sum().round(1)\n",
    "        frames_sma['bph'] = frames_sma['bph'].rolling(rolling_num, min_periods=1).sum().round(1)\n",
    "        frames_sma['precip'] = frames_sma['precip'].rolling(rolling_num, min_periods=1).sum().round(1)\n",
    "    else:\n",
    "        frames_sma['mirid bug'] = frames_sma['mirid bug'].rolling(rolling_num, min_periods=1).mean().round(1)\n",
    "        frames_sma['bph'] = frames_sma['bph'].rolling(rolling_num, min_periods=1).mean().round(1)\n",
    "        frames_sma['precip'] = frames_sma['precip'].rolling(rolling_num, min_periods=1).mean().round(1)\n",
    "   \n",
    "    for i in range(7,12):\n",
    "        frames_sma[h_name[i]] = frames_sma[h_name[i]].rolling(rolling_num, min_periods=1).mean().round(1)\n",
    "\n",
    "    constant_subset = frames_sma[::our_rate] \n",
    "    return constant_subset\n",
    "\n",
    "#################-- creat_dataset  --#################\n",
    "def creat_dataset(st_BPH = 'ALL', y_1='2015', y_2='2019', m_avr='ALL'):\n",
    "    for i in range(len(data_list_st_num)):\n",
    "        st = i\n",
    "        file_name = df_st['nameEng'][data_list_st_num[st]]\n",
    "        name_input = file_name\n",
    "        name_locals = 'stN_' + file_name\n",
    "\n",
    "        # File name and Path\n",
    "        csv_file = cwd + path_adress + name_input + '.csv'\n",
    "        # index_col=0 , index_col=None\n",
    "        dataset = pd.read_csv(csv_file, header=0,\n",
    "                              index_col=0, encoding=\"TIS-620\")\n",
    "\n",
    "        if m_avr == '3-DAY-All-':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,3,1,'mean')\n",
    "        elif m_avr == '7-DAY-All':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,7,1,'mean')\n",
    "        elif m_avr == '14-DAY-All':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,14,1,'mean')\n",
    "        elif m_avr == '3-DAY-Sampling':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,3,3,'sum')\n",
    "        elif m_avr == '7-DAY-Sampling':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,7,7,'sum')\n",
    "        elif m_avr == '14-DAY-Sampling':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,14,14,'sum')\n",
    "        else :\n",
    "            locals()[name_locals] = dataset\n",
    "            # print(f'Dataframe name_station: {st+1 , name_input}')\n",
    "        print('wait......')\n",
    "        del dataset\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    date_start = y_1 + '-01' + '-01'\n",
    "    date_stop = y_2 + '-12' + '-31'\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "    if st_BPH != 'ALL' :\n",
    "        # file_name = df_st['nameEng'][data_list_st_num[st_BPH]]\n",
    "        file_name = st_BPH\n",
    "        locals_input = 'stN_' + file_name\n",
    "        print(locals_input)\n",
    "        dataset_st = locals()[locals_input].loc[date_start:date_stop]\n",
    "        frames_st = dataset_st\n",
    "    else:\n",
    "        m = 0\n",
    "        for j in range(len(data_list_st_num)):\n",
    "            # for j in range(df_st.shape[0]):\n",
    "            # file_name = df_st['nameEng'][j]\n",
    "            file_name = df_st['nameEng'][data_list_st_num[j]]\n",
    "            locals_input = 'stN_' + file_name\n",
    "            print(locals_input)\n",
    "            dataset_st = locals()[locals_input].loc[date_start:date_stop]\n",
    "            clear_output(wait=True)\n",
    "            if m == 0:\n",
    "                frames_st = dataset_st\n",
    "                m = m+1\n",
    "                print(m)\n",
    "            else:\n",
    "                frames_st = [frames_st, dataset_st]\n",
    "                frames_st = pd.concat(frames_st)\n",
    "\n",
    "    # dataset    \n",
    "    frames = frames_st\n",
    "    return frames\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#################-- Drop Colum  --#################\n",
    "def drop_col(df_drop,drop = ['address']):\n",
    "    for j in range(len(drop)):\n",
    "        df_drop = df_drop.drop(drop[j], axis=1)\n",
    "    return df_drop\n",
    "\n",
    "#################--      Plot        --#################\n",
    "def plot_data(frames_train,plot_cols,df_name,start='2015',end='2017',st = 'ALL'):\n",
    "    #ตรวจสอบข้อมูล dataset \n",
    "    if st == 'ALL':\n",
    "        df_plot = frames_train[plot_cols]\n",
    "    else:\n",
    "        df_plot = frames_train[plot_cols].loc[start:end]\n",
    "    \n",
    "    plt.figure()    \n",
    "    # df_plot.plot(lw=1,grid=True,figsize=(13,30),subplots=True)\n",
    "    # df_plot.plot(lw=1,grid=True,subplots=True)\n",
    "    # df_plot.plot(marker='.',grid=True,linestyle = 'solid',subplots=True)\n",
    "    df_plot.plot(marker='.',grid=True,linestyle = 'solid')\n",
    "    plt.xlabel('Date time-'+ df_name)\n",
    "    plt.legend()\n",
    "    # plt.show()   \n",
    "\n",
    "#################--   get list of folders in directory   --#################\n",
    "dir_path = cwd + \"/Export_lstm/model/\"\n",
    "def get_list_folder():\n",
    "    folder_list = os.listdir(dir_path)\n",
    "    return folder_list\n",
    "\n",
    "def get_file_model(file_name):\n",
    "    txt = dir_path+file_name+\"/Training_model_data_discription.txt\"\n",
    "    string_data = open(txt,\"r\").read()\n",
    "    list_txt = list(string_data.split(\"\\n\"))\n",
    "    return list_txt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf1bcfc5d524e3c8ca533009e3ade8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='station :', options=('ALL', 'Chachoengsao Rice research Ce…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200ce3ca88f34980ac1cdd569748eda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output()), _titles={'0': '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#########################################-- Dashboard Input Data--#########################################\n",
    "ALL = 'ALL'\n",
    "\n",
    "def unique_sorted_values_plus_ALL(array):\n",
    "    unique = array.unique().tolist()\n",
    "    unique.sort()\n",
    "    unique.insert(0, ALL)\n",
    "    return unique\n",
    "\n",
    "#################-- DataFrame station --#################\n",
    "output_df_st = widgets.Output()\n",
    "def dropdown_station_eventhandler(change):\n",
    "    output_df_st.clear_output()\n",
    "    with output_df_st:  \n",
    "        if (change.new == ALL):\n",
    "            display(df_st)\n",
    "        else:\n",
    "            display(df_st[df_st.nameEng == change.new])\n",
    "\n",
    "#################-- DataFrame station --#################\n",
    "output_df_st_predict = widgets.Output()\n",
    "def dropdown_station_predict_eventhandler(change):\n",
    "    output_df_st_predict.clear_output()\n",
    "    with output_df_st_predict:  \n",
    "        if (change.new == ALL):\n",
    "            display(df_st)\n",
    "        else:\n",
    "            display(df_st[df_st.nameEng == change.new])\n",
    "\n",
    "#################-- load data  --#################\n",
    "\n",
    "\n",
    "output_df_train = widgets.Output()\n",
    "output_df_val = widgets.Output()\n",
    "output_df_test = widgets.Output()\n",
    "output_df_col = widgets.Output()\n",
    "def clicked_load(b):\n",
    "    global df_out_train\n",
    "    global df_out_val\n",
    "    global df_out_test\n",
    "    global selected_data \n",
    "    output_df_train.clear_output()\n",
    "    output_df_val.clear_output()\n",
    "    output_df_test.clear_output()\n",
    "    output_df_col.clear_output()\n",
    "    with output_df_train:\n",
    "        df_out_train = creat_dataset(dropdown_name_st.value,dropdown_year_train1.value,dropdown_year_train2.value,dropdown_sampling_data.value)\n",
    "        display(df_out_train)\n",
    "        # display(creat_dataset(dropdown_name_st.value,dropdown_year_train1.value,dropdown_year_train2.value,dropdown_sampling_data.value))\n",
    "    with output_df_val:\n",
    "        df_out_val = creat_dataset(dropdown_name_st.value,dropdown_year_val1.value,dropdown_year_val2.value,dropdown_sampling_data.value)\n",
    "        display(df_out_val)\n",
    "        # display(creat_dataset(dropdown_name_st.value,dropdown_year_val1.value,dropdown_year_val2.value,dropdown_sampling_data.value))\n",
    "    with output_df_test:\n",
    "        df_out_test = creat_dataset(dropdown_name_st.value,dropdown_year_test1.value,dropdown_year_test2.value,dropdown_sampling_data.value)\n",
    "        display(df_out_test)\n",
    "        # display(creat_dataset(dropdown_name_st.value,dropdown_year_test1.value,dropdown_year_test2.value,dropdown_sampling_data.value))\n",
    "    with output_df_col:\n",
    "        checkbox_objects = []\n",
    "        # global selected_data\n",
    "        data = sorted(set(chek_list(df_out_train)),reverse=True)     \n",
    "        names = []\n",
    "        for key in data:\n",
    "                checkbox_objects.append(widgets.Checkbox(value=True, description=key))\n",
    "                names.append(key)\n",
    " \n",
    "        arg_dict = {names[i]: checkbox for i, checkbox in enumerate(checkbox_objects)}\n",
    "        ui = widgets.HBox(children=checkbox_objects)\n",
    "        display(ui)\n",
    "             \n",
    "        selected_data = []\n",
    "        def select_data(**kwargs):\n",
    "            selected_data.clear()\n",
    "            for key in kwargs:\n",
    "                if kwargs[key] is True:\n",
    "                    selected_data.append(key)\n",
    "            print(selected_data)\n",
    "\n",
    "        widgets.interactive_output(select_data, arg_dict)\n",
    "    \n",
    " \n",
    "#################-- Export data  --#################\n",
    "df_train = widgets.Output()\n",
    "df_val = widgets.Output()\n",
    "df_test = widgets.Output()\n",
    "def clicked_export(b):\n",
    "    df_train.clear_output()\n",
    "    df_val.clear_output()\n",
    "    df_test.clear_output()\n",
    "    global frames_train\n",
    "    global frames_val\n",
    "    global frames_test\n",
    "    with df_train:\n",
    "        frames_train = drop_col(df_out_train,selected_data) \n",
    "        display(frames_train)\n",
    "    with df_val:\n",
    "        frames_val = drop_col(df_out_val,selected_data) \n",
    "        display(frames_val)  \n",
    "    with df_test:\n",
    "        frames_test = drop_col(df_out_test,selected_data) \n",
    "        display(frames_test)  \n",
    "\n",
    "def chek_list(df_ch):\n",
    "        data_col = set(df_ch.columns)\n",
    "        return data_col      \n",
    "\n",
    "#################----------------------------- DashBoard display ----------------------------------#################\n",
    "dropdown_name_st = widgets.Dropdown(options = unique_sorted_values_plus_ALL(df_st.nameEng),description='station :')\n",
    "dropdown_sampling_data = widgets.Dropdown(options = ['All','3-DAY-All','7-DAY-All','14-DAY-All','3-DAY-Sampling','7-DAY-Sampling','14-DAY-Sampling'],valure = 'All-DAY',description='Sampling :')\n",
    "dropdown_year_train1 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],description='Year-train :')\n",
    "dropdown_year_train2 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2017')\n",
    "dropdown_year_val1 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2018',description='Year-val :')\n",
    "dropdown_year_val2 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2018')\n",
    "dropdown_year_test1 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2019',description='Year-test :')\n",
    "dropdown_year_test2 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2019')\n",
    "\n",
    "\n",
    "button_download = widgets.Button(description='load data',disabled=False,button_style='success',tooltip='Click me',icon='check')\n",
    "button_export = widgets.Button(description='export dataset',disabled=False,button_style='success',tooltip='Click me',icon='check')\n",
    "\n",
    "button_download.on_click(clicked_load)\n",
    "button_export.on_click(clicked_export)\n",
    "\n",
    "dropdown_name_st.observe(dropdown_station_eventhandler, names='value')\n",
    "\n",
    "\n",
    "item_layout = widgets.Layout(margin='0 0 15px 0')\n",
    "input_widgets_row1 = widgets.HBox([dropdown_name_st,dropdown_year_train1,dropdown_year_train2,dropdown_year_val1,dropdown_year_val2,dropdown_year_test1,dropdown_year_test2],layout = item_layout)\n",
    "input_widgets_row2 = widgets.HBox([dropdown_sampling_data,button_download,button_export],layout = item_layout)\n",
    "\n",
    "tab_dataset = widgets.Tab([output_df_st,output_df_train,output_df_val,output_df_test,output_df_col,df_train,df_val,df_test])\n",
    "tab_dataset.set_title(0, 'Dataset station')\n",
    "tab_dataset.set_title(1, 'train-data')\n",
    "tab_dataset.set_title(2, 'validation-data')\n",
    "tab_dataset.set_title(3, 'test-data')\n",
    "tab_dataset.set_title(4, 'Drop-data')\n",
    "tab_dataset.set_title(5, 'Dataset-train')\n",
    "tab_dataset.set_title(6, 'Dataset-validation')\n",
    "tab_dataset.set_title(7, 'Dataset-test')\n",
    "dashboard_dataset = widgets.VBox([input_widgets_row1,input_widgets_row2])\n",
    "display(dashboard_dataset)\n",
    "display(tab_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "def plot_data(frames_train,df_name):\n",
    "    #ตรวจสอบข้อมูล dataset \n",
    "    plt.figure()\n",
    "    df_plot = frames_train\n",
    "    df_plot.plot(lw=1,grid=True,figsize=(15,7),subplots=True)\n",
    "    plt.xlabel('time-(day)'+ df_name)\n",
    "    plt.legend()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(Epoc,n_input,n_output):\n",
    "\n",
    "    #Multivariate = [temp,precip,humidity,bph] 4\n",
    "    #Multivariate = [temp,precip,humidity,day,month,bph] 6\n",
    "    #Multivariate = [temp,precip,humidity,day,month,bph] + [rice] \n",
    "\n",
    "    values_train = frames_train.values    \n",
    "    n_features = frames_train.shape[1]\n",
    "\n",
    "    values_validation = frames_val.values\n",
    "    plot_data(frames_train,'  Dataset')\n",
    "\n",
    "    # In_day   Out_day\n",
    "    #    7      1\n",
    "    #    7      3\n",
    "    #    7      5\n",
    "    #    7      7\n",
    "    #   14      1\n",
    "    #   14      3\n",
    "    #   14      5\n",
    "    #   14      7\n",
    "    #   21      1\n",
    "    #   21      3\n",
    "    #   21      5\n",
    "    #   21      7\n",
    "    Export_folder_name = \"/Users/MSI-1016TH/Documents/ผลการทดลอง/\"\n",
    "    # Export_folder_name = \"/home/np13-sut-pc/Documents/ผลการทดลอง/\"\n",
    "    Epochs = int(Epoc)\n",
    "    n_day = int(n_input)\n",
    "    n_out = int(n_output)\n",
    "\n",
    "    #train data\n",
    "    # ensure all data is float\n",
    "    values = values_train.astype('float32')\n",
    "    # normalize features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled = scaler.fit_transform(values)\n",
    "\n",
    "    # frame as supervised learning\n",
    "    reframed = series_to_supervised(scaled, n_day, n_out)\n",
    "    # print(reframed.shape)\n",
    "    # print(reframed.head())\n",
    "\n",
    "    # predict datasets\n",
    "    values = reframed.values\n",
    "    train = values\n",
    "\n",
    "    #input \n",
    "    n_obs = n_day * n_features\n",
    "    # train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
    "    train_X, train_y = train[:, :n_obs], train[:, -1]\n",
    "    print(train_X.shape, len(train_X), train_y.shape)  #for train\n",
    "\n",
    "\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], n_day, n_features))\n",
    "    print(train_X.shape, train_y.shape)\n",
    "\n",
    "    #validation data\n",
    "    # ensure all data is float\n",
    "    values = values_validation.astype('float32')\n",
    "    # normalize features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled = scaler.fit_transform(values)\n",
    "\n",
    "    # frame as supervised learning\n",
    "    reframed = series_to_supervised(scaled, n_day, n_out)\n",
    "\n",
    "    # predict datasets\n",
    "    values = reframed.values\n",
    "    test = values\n",
    "\n",
    "    #output \n",
    "    n_obs = n_day * n_features\n",
    "    # test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "    test_X, test_y = test[:, :n_obs], test[:, -1]\n",
    "    print(test_X.shape, len(test_X), test_y.shape)  #for train\n",
    "\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    test_X = test_X.reshape((test_X.shape[0], n_day, n_features))\n",
    "    print(test_X.shape, test_y.shape)\n",
    "\n",
    "\n",
    "    ##------train-----## \n",
    "    # Model A LSTM \n",
    "    model = tf.keras.models.Sequential([\n",
    "                    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "                    keras.layers.LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2]),activation='relu'),  #relu tanh\n",
    "                    keras.layers.Dense(units=1)\n",
    "        ])\n",
    "    # Model B Bidirectional-LSTM\n",
    "    # model = tf.keras.models.Sequential([\n",
    "    #                   # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    #                   keras.layers.Bidirectional(layers.LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2]),activation='relu')),  #relu tanh\n",
    "    #                   keras.layers.Dense(units=1)\n",
    "    #     ])\n",
    "    # Model C Stacked_LSTM\n",
    "    # model = tf.keras.models.Sequential([\n",
    "    #                   # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    #                   keras.layers.LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2]),return_sequences=True,activation='relu'),\n",
    "    #                   keras.layers.LSTM(50, activation='relu'),\n",
    "    #                   keras.layers.Dense(units=1)\n",
    "    #             ])\n",
    "                \n",
    "    Optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "    model.compile(Optimizer, loss='mae', metrics=['accuracy'])\n",
    "    # model.summary()\n",
    "\n",
    "    # fit network\n",
    "    batch_size = 128\n",
    "    history = model.fit(train_X, train_y, \n",
    "                        epochs=Epochs, \n",
    "                        batch_size=batch_size, \n",
    "                        validation_data=(test_X, test_y), \n",
    "                        verbose=2, \n",
    "                        # callbacks=[cp_callback,es_callback], \n",
    "                        shuffle=False)\n",
    "\n",
    "    # plot history\n",
    "    from matplotlib import pyplot as plt1\n",
    "    def plot_loss(history):\n",
    "        plt1.figure(figsize=(10,7))\n",
    "        plt1.plot(history.history['loss'], label='train')\n",
    "        # plt.plot(history.history['val_loss'], label='val')\n",
    "        plt1.xlabel('Epoch')\n",
    "        plt1.ylabel('Error')\n",
    "        plt1.legend()\n",
    "        plt1.grid(True)\n",
    "    #     plt.show()\n",
    "\n",
    "    plot_loss(history)\n",
    "\n",
    "    plot_name_T = 'output model A '+str(n_day)+'-'+str(n_out)+'-'+str(Epochs)+'-'+str(len(list(frames_train.columns)))+'V'+'-T'+'.png'\n",
    "    plt1.savefig(Export_folder_name + plot_name_T)\n",
    "    plt1.close()\n",
    "\n",
    "    #####################################################################################################\n",
    "\n",
    "    values_predict = frames_test.values\n",
    "    n_features = frames_test.shape[1]\n",
    "    df = frames_test.reset_index()\n",
    "    date_time_predict = pd.to_datetime(df.pop('date'))\n",
    "\n",
    "    # ensure all data is float\n",
    "    values = values_predict.astype('float32')\n",
    "    # normalize features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled = scaler.fit_transform(values)\n",
    "        \n",
    "    # frame as supervised learning\n",
    "    reframed = series_to_supervised(scaled, n_day, n_out)\n",
    "    # print(reframed.shape)\n",
    "    # print(reframed.head())\n",
    "        \n",
    "    # predict datasets\n",
    "    values = reframed.values\n",
    "    # n_train_day = int(values.shape[0]*0.9)\n",
    "    # train = values[:n_train_day, :]\n",
    "    test = values\n",
    "        \n",
    "    # predict into input and outputs\n",
    "    n_obs = n_day * n_features\n",
    "    # test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "    test_X, test_y = test[:, :n_obs], test[:, -1]\n",
    "    print(test_X.shape, len(test_X), test_y.shape)\n",
    "        \n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    test_X = test_X.reshape((test_X.shape[0], n_day, n_features))\n",
    "    print(test_X.shape, test_y.shape)\n",
    "        \n",
    "    # # make a prediction\n",
    "    yhat = model.predict(test_X,verbose=0)\n",
    "    test_X_reshape = test_X.reshape((test_X.shape[0], n_day*n_features)) \n",
    "\n",
    "    from cProfile import label\n",
    "    from numpy import concatenate\n",
    "\n",
    "    # invert scaling for forecast\n",
    "    # inv_yhat = concatenate((yhat, test_X[:, -29:]), axis=1)\n",
    "    inv_yhat = concatenate((test_X_reshape[:, :(n_features-1)], yhat), axis=1)\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,-1]\n",
    "\n",
    "    # invert scaling for actual\n",
    "    test_y_reshape = test_y.reshape((len(test_y), 1))\n",
    "    # inv_y = concatenate((test_y, test_X[:, -29:]), axis=1)\n",
    "    inv_y = concatenate((test_X_reshape[:, :(n_features-1)], test_y_reshape), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,-1]\n",
    "\n",
    "\n",
    "    ##### plot and save ###\n",
    "\n",
    "    df_out = pd.DataFrame(list(date_time_predict[n_day+n_out-1:]),columns=['date'])\n",
    "    df_out['actual'] = pd.DataFrame(list(inv_y))\n",
    "    df_out['predict'] = pd.DataFrame(list(inv_yhat))\n",
    "\n",
    "    # calculate RMSE\n",
    "    m = tf.keras.metrics.RootMeanSquaredError()\n",
    "    m.update_state([df_out['actual']], [df_out['predict']])\n",
    "    rmse_all = m.result().numpy()\n",
    "    print('RMSE-all: %.3f' % rmse_all)\n",
    "\n",
    "\n",
    "    d_range = 365-n_day-n_out-1\n",
    "    pre_start = 0 \n",
    "\n",
    "    pre_stop = pre_start+d_range\n",
    "\n",
    "    # calculate RMSE\n",
    "    m = tf.keras.metrics.RootMeanSquaredError()\n",
    "    m.update_state(df_out['actual'][pre_start:pre_stop], df_out['predict'][pre_start:pre_stop])\n",
    "    rmse = m.result().numpy()\n",
    "    print('RMSE: %.3f' % rmse)\n",
    "\n",
    "    strings = list(frames_test.columns)\n",
    "    textstr = '\\n'.join(strings[:12])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_figheight(6)\n",
    "    fig.set_figwidth(10)\n",
    "\n",
    "    ax.plot(df_out['date'][pre_start:pre_stop],df_out['actual'][pre_start:pre_stop],label='actual')\n",
    "    ax.plot(df_out['date'][pre_start:pre_stop],df_out['predict'][pre_start:pre_stop],label='predict')\n",
    "\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_title('Multivariate-LSTM model-A' +'  RMSE: %.2f' % rmse)\n",
    "    ax.set_xlabel('time (day) '+'(%d-'%n_day+'%d-'%n_out+'Epoch %d)'%Epochs)\n",
    "    ax.set_ylabel('BPH Volume')\n",
    "    ax.grid(True)\n",
    "\n",
    "    # these are matplotlib.patch.Patch properties\n",
    "    props = dict(boxstyle='round', alpha=0.5)\n",
    "\n",
    "    # place a text box in upper left in axes coords\n",
    "    ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "            verticalalignment='top', bbox=props)\n",
    "\n",
    "    file_name = 'output model A '+str(n_day)+'-'+str(n_out)+'-'+str(Epochs)+'-'+str(len(strings))+'V'\n",
    "    # plot_name = 'output model A '+str(n_day)+'-'+str(n_out)+'-'+str(Epochs)+'-'+str(len(strings))+'V'+'.png'\n",
    "    plot_name = file_name +'.png'\n",
    "    plt.savefig(Export_folder_name + plot_name)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    #Output : file name and Path Save file\n",
    "    # file_name_save = 'output model A '+str(n_day)+'-'+str(n_out)+'-'+str(Epochs)+'-'+str(len(strings))+'V'+'.csv'\n",
    "    file_name_save = file_name +'.csv'\n",
    "    dataset_path_save = Export_folder_name + file_name_save\n",
    "    df_out.to_csv(dataset_path_save, index=False, encoding=\"TIS-620\")\n",
    "    print(\"{} {}\" .format(dataset_path_save,len(df_out)))\n",
    "    del df_out,model\n",
    "    return  str(file_name),str(rmse),str(rmse_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs = [300]\n",
    "n_in = [7,14,21]\n",
    "n_out = [1,3,5,7]\n",
    "\n",
    "# n_in = [7]\n",
    "# n_out = [1]\n",
    "list_name_result = []\n",
    "list_rmse_result = []\n",
    "list_rmse_all_result = []\n",
    "for i in range(len(Epochs)):\n",
    "    for j in range(len(n_in)):\n",
    "         for k in range(len(n_out)):\n",
    "            result_train = train_loop(Epochs[i],n_in[j],n_out[k])\n",
    "            print(result_train[0])\n",
    "            print(result_train[1])\n",
    "            list_name_result.append(result_train[0]) \n",
    "            list_rmse_result.append(result_train[1])\n",
    "            list_rmse_all_result.append(result_train[2]) \n",
    "\n",
    "d = {'name': list_name_result, 'rmse': list_rmse_result,'rmse_all': list_rmse_all_result}\n",
    "df_result = pd.DataFrame(data=d)\n",
    "\n",
    "Export_folder_name = \"/Users/MSI-1016TH/Documents/ผลการทดลอง/\"\n",
    "# Export_folder_name = \"/home/np13-sut-pc/Documents/ผลการทดลอง/\"\n",
    "file_name = 'output model LSTM rmse run 1'\n",
    "file_name_save = file_name +'.csv'\n",
    "dataset_path_save = Export_folder_name + file_name_save\n",
    "df_result.to_csv(dataset_path_save, index=False, encoding=\"TIS-620\")\n",
    "print(\"{} {}\" .format(dataset_path_save,len(df_result)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb01e479798b652ff2decc92044ac783e2e74b0a89b91ea27564b1f393a8174f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
