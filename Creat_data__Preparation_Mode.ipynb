{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.7\n",
      "1.21.2\n",
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "# import \n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "# import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# import tensorflow \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "import platform\n",
    "print(platform.python_version())\n",
    "print(np.__version__)\n",
    "print(tf.__version__)\n",
    "\n",
    "# tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All low RiceCenter 34 station\n"
     ]
    }
   ],
   "source": [
    "# File name and Path\n",
    "cwd = os.getcwd()\n",
    "path_adress = \"/Import_Dataset/\"\n",
    "\n",
    "# ข้อมูล พิกัดสถานที่เเละรายชื้ออ้างอิงสถานที่ตรวจวัด\n",
    "file_name_st = 'Data_lat_long_Rice research Center'\n",
    "csv_file_st = cwd + path_adress + file_name_st + '.csv'\n",
    "\n",
    "df_st = pd.read_csv(csv_file_st)\n",
    "print(\"All low RiceCenter {} station\" .format(df_st.shape[0]))\n",
    "\n",
    "data_list_st_num = list(range(0, len(df_st)))\n",
    "data_list_st_name = []\n",
    "data_list_st_name = df_st['nameEng'].values.tolist()\n",
    "data_list_st_lat = df_st['Latitude'].values.tolist()\n",
    "data_list_st_long = df_st['Longitude'].values.tolist()\n",
    "\n",
    "#################-- Moving Averag data --#################\n",
    "def mavr_dataset(frames_sma,rolling_num,our_rate):    \n",
    "    h_name = list(frames_sma)\n",
    "    frames_sma['mirid bug'] = frames_sma['mirid bug'].rolling(rolling_num, min_periods=1).sum().round(1)\n",
    "    frames_sma['bph'] = frames_sma['bph'].rolling(rolling_num, min_periods=1).sum().round(1)\n",
    "    # frames_sma['mirid bug'] = frames_sma['mirid bug'].rolling(rolling_num, min_periods=1).mean().round(1)\n",
    "    # frames_sma['bph'] = frames_sma['bph'].rolling(rolling_num, min_periods=1).mean().round(1)\n",
    "    for i in range(7,14):\n",
    "        frames_sma[h_name[i]] = frames_sma[h_name[i]].rolling(rolling_num, min_periods=1).mean().round(1)\n",
    "\n",
    "    constant_subset = frames_sma[::our_rate] \n",
    "    return constant_subset\n",
    "\n",
    "#################-- creat_dataset  --#################\n",
    "def creat_dataset(st_BPH = 'ALL', y_1='2015', y_2='2019', m_avr='ALL-DAY'):\n",
    "    for i in range(len(data_list_st_num)):\n",
    "        st = i\n",
    "        file_name = df_st['nameEng'][data_list_st_num[st]]\n",
    "        name_input = file_name\n",
    "        name_locals = 'stN_' + file_name\n",
    "\n",
    "        # File name and Path\n",
    "        csv_file = cwd + path_adress + name_input + '.csv'\n",
    "        # index_col=0 , index_col=None\n",
    "        dataset = pd.read_csv(csv_file, header=0,\n",
    "                              index_col=0, encoding=\"TIS-620\")\n",
    "\n",
    "        if m_avr == '3-DAY':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,3,3)\n",
    "        elif m_avr == '7-DAY':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,7,7)\n",
    "        else :\n",
    "            locals()[name_locals] = mavr_dataset(dataset,7,1)\n",
    "            # print(f'Dataframe name_station: {st+1 , name_input}')\n",
    "        print('wait......')\n",
    "        del dataset\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    date_start = y_1 + '-01' + '-01'\n",
    "    date_stop = y_2 + '-12' + '-31'\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "    if st_BPH != 'ALL' :\n",
    "        # file_name = df_st['nameEng'][data_list_st_num[st_BPH]]\n",
    "        file_name = st_BPH\n",
    "        locals_input = 'stN_' + file_name\n",
    "        print(locals_input)\n",
    "        dataset_st = locals()[locals_input].loc[date_start:date_stop]\n",
    "        frames_st = dataset_st\n",
    "    else:\n",
    "        m = 0\n",
    "        for j in range(len(data_list_st_num)):\n",
    "            # for j in range(df_st.shape[0]):\n",
    "            # file_name = df_st['nameEng'][j]\n",
    "            file_name = df_st['nameEng'][data_list_st_num[j]]\n",
    "            locals_input = 'stN_' + file_name\n",
    "            print(locals_input)\n",
    "            dataset_st = locals()[locals_input].loc[date_start:date_stop]\n",
    "            clear_output(wait=True)\n",
    "            if m == 0:\n",
    "                frames_st = dataset_st\n",
    "                m = m+1\n",
    "                print(m)\n",
    "            else:\n",
    "                frames_st = [frames_st, dataset_st]\n",
    "                frames_st = pd.concat(frames_st)\n",
    "\n",
    "    # dataset    \n",
    "    frames = frames_st\n",
    "    return frames\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#################-- Drop Colum  --#################\n",
    "def drop_col(df_drop,drop = ['address']):\n",
    "    for j in range(len(drop)):\n",
    "        df_drop = df_drop.drop(drop[j], axis=1)\n",
    "    return df_drop\n",
    "\n",
    "#################--      Plot        --#################\n",
    "def plot_data(frames_train,plot_cols,df_name,start='2015',end='2017'):\n",
    "    #ตรวจสอบข้อมูล dataset \n",
    "    \n",
    "    df_plot = frames_train[plot_cols].loc[start:end]\n",
    "    plt.figure()    \n",
    "    # df_plot.plot(lw=1,grid=True,figsize=(13,30),subplots=True)\n",
    "    # df_plot.plot(lw=1,grid=True,subplots=True)\n",
    "    # df_plot.plot(marker='.',grid=True,linestyle = 'solid',subplots=True)\n",
    "    df_plot.plot(marker='.',grid=True,linestyle = 'solid')\n",
    "    plt.xlabel('Date time-'+ df_name)\n",
    "    plt.legend()\n",
    "    # plt.show()   \n",
    "\n",
    "#################--                  --#################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################-- convert series to supervised learning --#################\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "\n",
    "#################--          data_preprocess        --#################\n",
    "def data_preprocess(data_frames,n_day,n_out):\n",
    "      values_df = data_frames.values    #ตัด header กับ idx ออก เป็น array matrix\n",
    "      n_features = data_frames.shape[1]\n",
    "      # print(n_features)\n",
    "      \n",
    "      # ensure all data is float\n",
    "      values = values_df.astype('float32')\n",
    "      # normalize features\n",
    "      scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "      scaled = scaler.fit_transform(values)\n",
    "\n",
    "      # frame as supervised learning\n",
    "      reframed = series_to_supervised(scaled, n_day, n_out)\n",
    "      # print(reframed.shape)\n",
    "      # print(reframed.head())\n",
    "\n",
    "      # datasets\n",
    "      values = reframed.values\n",
    "\n",
    "      #input \n",
    "      n_obs = n_day * n_features\n",
    "      dataset_X, dataset_y = values[:, :n_obs], values[:, -1]\n",
    "      # print(dataset_X.shape, len(dataset_X), dataset_y.shape) \n",
    "\n",
    "      # reshape input to be 3D [samples, timesteps, features]\n",
    "      dataset_X = dataset_X.reshape((dataset_X.shape[0], n_day, n_features))\n",
    "      # print(dataset_X.shape, dataset_y.shape)\n",
    "\n",
    "      return dataset_X,dataset_y\n",
    "       \n",
    "#################--           create_model           --#################\n",
    "def create_model(units,shape_1,shape_2,activation = 'relu',optimi = 0.0001,loss_input = 'mae',metrics_input = 'accuracy'):\n",
    "      model = tf.keras.models.Sequential([\n",
    "                  # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "                  keras.layers.LSTM(units=units, input_shape=(shape_1, shape_2),activation=activation),\n",
    "                  # keras.BatchNormalization(),\n",
    "                  keras.layers.Dense(units=1)\n",
    "            ])            \n",
    "      Optimizer = tf.keras.optimizers.Adam(optimi)\n",
    "      model.compile(Optimizer, loss=loss_input, metrics= metrics_input)\n",
    "      # model.compile(Optimizer, loss=loss_input)\n",
    "      # model.summary()\n",
    "      return model\n",
    "\n",
    "\n",
    "def fit_model(train_x, train_y,val_x, val_y,Epochs,batch_size):\n",
    "      # fit network\n",
    "           \n",
    "      # Load the TensorBoard notebook extension\n",
    "      %load_ext tensorboard \n",
    "      log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "      tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "      tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "      history = model_out.fit(train_x, train_y, \n",
    "                            epochs=Epochs, \n",
    "                            batch_size=batch_size, \n",
    "                            validation_data=(val_x, val_y), \n",
    "                            verbose=2, \n",
    "                            # callbacks=[cp_callback,es_callback], \n",
    "                            # callbacks=[cp_callback],\n",
    "                            callbacks=[tensorboard_callback],\n",
    "                            shuffle=False)\n",
    "      # clear_output(wait=True)\n",
    "      return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdaf1cf199d648de88566ec8d62a7845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='station :', options=('ALL', 'Chachoengsao Rice research Ce…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bce1ead834943e58b682ad4605b5373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output()), _titles={'0': '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dabcc3052954db1996b4c5fd3876279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=500, continuous_update=False, description='Epochs :', max=5000, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b229cafd234fe1a2c448c11ea944d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output()), _titles={'0': 'model.summary', '1': 'model_fit'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#################-- Dashboard --#################\n",
    "\n",
    "ALL = 'ALL'\n",
    "def unique_sorted_values_plus_ALL(array):\n",
    "    unique = array.unique().tolist()\n",
    "    unique.sort()\n",
    "    unique.insert(0, ALL)\n",
    "    return unique\n",
    "\n",
    "#################-- DataFrame station --#################\n",
    "output_df_st = widgets.Output()\n",
    "def dropdown_station_eventhandler(change):\n",
    "    output_df_st.clear_output()\n",
    "    with output_df_st:  \n",
    "        if (change.new == ALL):\n",
    "            display(df_st)\n",
    "        else:\n",
    "            display(df_st[df_st.nameEng == change.new])\n",
    "\n",
    "#################-- select sampling --#################\n",
    "def dropdown_sampling_eventhandler(change):\n",
    "    return dropdown_sampling_data\n",
    "\n",
    "#################-- select year train --#################\n",
    "def dropdown_year_t1_eventhandler(change):\n",
    "    return dropdown_year_train1\n",
    "def dropdown_year_t2_eventhandler(change):\n",
    "    return dropdown_year_train2\n",
    "def dropdown_year_v1_eventhandler(change):\n",
    "    return dropdown_year_val1\n",
    "def dropdown_year_v2_eventhandler(change):\n",
    "    return dropdown_year_val2\n",
    "def dropdown_year_ts1_eventhandler(change):\n",
    "    return dropdown_year_test1\n",
    "def dropdown_year_ts2_eventhandler(change):\n",
    "    return dropdown_year_test2\n",
    "\n",
    "#################-- select model --#################\n",
    "def Slider_Epochs_eventhandler(change):\n",
    "    return Slider_Epochs\n",
    "def Slider_batch_size_eventhandler(change):\n",
    "    return Slider_batch_size\n",
    "def Box_Units_eventhandler(change):\n",
    "    return Box_Units\n",
    "def Box_Optimizer_eventhandler(change):\n",
    "    return Box_Optimizer\n",
    "def dropdown_activation_eventhandler(change):\n",
    "    return dropdown_activation\n",
    "def dropdown_loss_eventhandler(change):\n",
    "    return dropdown_loss\n",
    "def Box_nday_eventhandler(change):\n",
    "    return Box_nday\n",
    "def Box_nout_eventhandler(change):\n",
    "    return Box_nout\n",
    "\n",
    "\n",
    "\n",
    "#################-- load data  --#################\n",
    "output_df_train = widgets.Output()\n",
    "output_df_val = widgets.Output()\n",
    "output_df_test = widgets.Output()\n",
    "output_df_col = widgets.Output()\n",
    "def clicked_load(b):\n",
    "   \n",
    "    global df_out_train\n",
    "    global df_out_val\n",
    "    global df_out_test\n",
    "    global selected_data\n",
    "    output_df_train.clear_output()\n",
    "    output_df_val.clear_output()\n",
    "    output_df_test.clear_output()\n",
    "    output_df_col.clear_output()\n",
    "    with output_df_train:\n",
    "        df_out_train = creat_dataset(dropdown_name_st.value,dropdown_year_train1.value,dropdown_year_train2.value,dropdown_sampling_data.value)\n",
    "        display(df_out_train)\n",
    "        # display(creat_dataset(dropdown_name_st.value,dropdown_year_train1.value,dropdown_year_train2.value,dropdown_sampling_data.value))\n",
    "    with output_df_val:\n",
    "        df_out_val = creat_dataset(dropdown_name_st.value,dropdown_year_val1.value,dropdown_year_val2.value,dropdown_sampling_data.value)\n",
    "        display(df_out_val)\n",
    "        # display(creat_dataset(dropdown_name_st.value,dropdown_year_val1.value,dropdown_year_val2.value,dropdown_sampling_data.value))\n",
    "    with output_df_test:\n",
    "        df_out_test = creat_dataset(dropdown_name_st.value,dropdown_year_test1.value,dropdown_year_test2.value,dropdown_sampling_data.value)\n",
    "        display(df_out_test)\n",
    "        # display(creat_dataset(dropdown_name_st.value,dropdown_year_test1.value,dropdown_year_test2.value,dropdown_sampling_data.value))\n",
    "    with output_df_col:\n",
    "        checkbox_objects = []\n",
    "        # global selected_data\n",
    "        data = set(chek_list(df_out_train))    \n",
    "        names = []\n",
    "        for key in data:\n",
    "                checkbox_objects.append(widgets.Checkbox(value=False, description=key))\n",
    "                names.append(key)\n",
    " \n",
    "        arg_dict = {names[i]: checkbox for i, checkbox in enumerate(checkbox_objects)}\n",
    "        ui = widgets.HBox(children=checkbox_objects)\n",
    "        display(ui)\n",
    "             \n",
    "        selected_data = []\n",
    "        def select_data(**kwargs):\n",
    "            selected_data.clear()\n",
    "            for key in kwargs:\n",
    "                if kwargs[key] is True:\n",
    "                    selected_data.append(key)\n",
    "            print(selected_data)\n",
    "\n",
    "        widgets.interactive_output(select_data, arg_dict)\n",
    "    \n",
    "    return df_out_train,df_out_val,df_out_test\n",
    "\n",
    "def chek_list(df_ch):\n",
    "        data_col = set(df_ch.columns)    \n",
    "        return data_col\n",
    "\n",
    "#################-- Export data  --#################\n",
    "df_train = widgets.Output()\n",
    "df_val = widgets.Output()\n",
    "df_test = widgets.Output()\n",
    "def clicked_export(b):\n",
    "    df_train.clear_output()\n",
    "    df_val.clear_output()\n",
    "    df_test.clear_output()\n",
    "    global frames_train\n",
    "    global frames_val\n",
    "    global frames_test\n",
    "    with df_train:\n",
    "        frames_train = drop_col(df_out_train,selected_data) \n",
    "        display(frames_train)\n",
    "    with df_val:\n",
    "        frames_val = drop_col(df_out_val,selected_data) \n",
    "        display(frames_val)  \n",
    "    with df_test:\n",
    "        frames_test = drop_col(df_out_test,selected_data) \n",
    "        display(frames_test)     \n",
    "\n",
    "\n",
    "#################-- create_model  --#################\n",
    "output_create_model = widgets.Output()\n",
    "def clicked_model(b):\n",
    "    output_create_model.clear_output()\n",
    "    global model_out\n",
    "    global train_X\n",
    "    global train_Y\n",
    "    global val_X\n",
    "    global val_Y\n",
    "    \n",
    "    \n",
    "    with output_create_model:\n",
    "        train_X, train_Y = data_preprocess(frames_train,Box_nday.value,Box_nout.value)\n",
    "        val_X,val_Y = data_preprocess(frames_val,Box_nday.value,Box_nout.value)              \n",
    "        model_out = create_model(Box_Units.value,train_X.shape[1],train_X.shape[2],dropdown_activation.value)\n",
    "        clear_output(wait=True) \n",
    "        print(model_out.summary())\n",
    "        print(train_X.shape)  \n",
    "        print(train_Y.shape) \n",
    "        \n",
    "#################-- fit_model  --#################\n",
    "output_fit_model = widgets.Output()   \n",
    "def clicked_fit(b):\n",
    "    output_fit_model.clear_output()\n",
    "    global history_out\n",
    "    \n",
    "    with output_fit_model:\n",
    "        # fit network\n",
    "        history_out = fit_model(train_X, train_Y,val_X, val_Y,Slider_Epochs.value,Slider_batch_size.value)\n",
    "        %tensorboard --logdir logs/fit\n",
    "        # Epochs = Slider_Epochs.value\n",
    "        # batch_size = Slider_batch_size.value\n",
    "        # history = model_out.fit(train_X, train_y, \n",
    "        #                     epochs=Epochs, \n",
    "        #                     batch_size=batch_size, \n",
    "        #                     validation_data=(val_X, val_y), \n",
    "        #                     verbose=2, \n",
    "        #                     # callbacks=[cp_callback,es_callback], \n",
    "        #                     # callbacks=[cp_callback],\n",
    "        #                 #   callbacks=[tensorboard_callback],\n",
    "        #                     shuffle=False)\n",
    "        \n",
    "        \n",
    "\n",
    "#################-- Plot data  --#################\n",
    "output_df_plot = widgets.Output()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################----------------------------- DashBoard  ----------------------------------#################\n",
    "dropdown_name_st = widgets.Dropdown(options = unique_sorted_values_plus_ALL(df_st.nameEng),description='station :')\n",
    "dropdown_sampling_data = widgets.Dropdown(options = ['All-DAY','3-DAY','7-DAY'],valure = 'All-DAY',description='Sampling :')\n",
    "dropdown_year_train1 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],description='Year-train :')\n",
    "dropdown_year_train2 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2017')\n",
    "dropdown_year_val1 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2018',description='Year-val :')\n",
    "dropdown_year_val2 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2018')\n",
    "dropdown_year_test1 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2019',description='Year-test :')\n",
    "dropdown_year_test2 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2019')\n",
    "\n",
    "button_download = widgets.Button(description='load data',disabled=False,button_style='success',tooltip='Click me',icon='check')\n",
    "button_export = widgets.Button(description='export dataset',disabled=False,button_style='success',tooltip='Click me',icon='check')\n",
    "button_model = widgets.Button(description='create_model',disabled=False,button_style='success',tooltip='Click me',icon='check')\n",
    "button_fit = widgets.Button(description='fit_model',disabled=False,button_style='success',tooltip='Click me',icon='check')\n",
    "\n",
    "\n",
    "Slider_Epochs = widgets.IntSlider(value=500,min=10,max=5000,step=10,description='Epochs :',disabled=False,continuous_update=False,orientation='horizontal', readout=True,readout_format='d')\n",
    "Slider_batch_size = widgets.IntSlider(value=128,min=16,max=256,step=8,description='Batch_size :',disabled=False,continuous_update=False,orientation='horizontal', readout=True,readout_format='d')\n",
    "Box_Units = widgets.IntText(value=50,description='units :',disabled=False)\n",
    "Box_Optimizer = widgets.FloatText(value=0.0001,step=0.0001,description='Optimizer :',disabled=False)\n",
    "dropdown_activation = widgets.Dropdown(options = ['relu','elu','sigmoid','tanh'],valure = 'elu',description='activation :')\n",
    "dropdown_loss = widgets.Dropdown(options = ['mae','mae'],valure = 'mae',description='loss :')\n",
    "Box_nday = widgets.IntText(value=7,description='n-day :',disabled=False)\n",
    "Box_nout = widgets.IntText(value=3,description='n-out :',disabled=False)\n",
    "\n",
    "button_download.on_click(clicked_load)\n",
    "button_export.on_click(clicked_export)\n",
    "button_model.on_click(clicked_model)\n",
    "button_fit.on_click(clicked_fit)\n",
    "\n",
    "dropdown_name_st.observe(dropdown_station_eventhandler, names='value')\n",
    "dropdown_sampling_data.observe(dropdown_sampling_eventhandler, names='value')\n",
    "dropdown_year_train1.observe(dropdown_year_t1_eventhandler, names='value')\n",
    "dropdown_year_train2.observe(dropdown_year_t2_eventhandler, names='value')\n",
    "dropdown_year_val1.observe(dropdown_year_v1_eventhandler, names='value')\n",
    "dropdown_year_val2.observe(dropdown_year_v2_eventhandler, names='value')\n",
    "dropdown_year_test1.observe(dropdown_year_ts1_eventhandler, names='value')\n",
    "dropdown_year_test2.observe(dropdown_year_ts2_eventhandler, names='value')\n",
    "Slider_Epochs.observe(Slider_Epochs_eventhandler, names='value')\n",
    "Slider_batch_size.observe(Slider_batch_size_eventhandler, names='value')\n",
    "Box_Units.observe(Box_Units_eventhandler, names='value')\n",
    "Box_Optimizer.observe(Box_Optimizer_eventhandler, names='value')\n",
    "dropdown_activation.observe(dropdown_activation_eventhandler, names='value')\n",
    "dropdown_loss.observe(dropdown_loss_eventhandler, names='value')\n",
    "Box_nday.observe(Box_nday_eventhandler, names='value')\n",
    "Box_nout.observe(Box_nout_eventhandler, names='value')\n",
    "\n",
    "\n",
    "item_layout = widgets.Layout(margin='0 0 15px 0')\n",
    "\n",
    "input_widgets_row1 = widgets.HBox([dropdown_name_st,dropdown_year_train1,dropdown_year_train2,dropdown_year_val1,dropdown_year_val2,dropdown_year_test1,dropdown_year_test2],layout = item_layout)\n",
    "input_widgets_row2 = widgets.HBox([dropdown_sampling_data,button_download,button_export],layout = item_layout)\n",
    "input_widgets_row3 = widgets.HBox([Slider_Epochs,Slider_batch_size,Box_Units,Box_Optimizer],layout = item_layout)\n",
    "input_widgets_row4 = widgets.HBox([dropdown_activation,dropdown_loss,Box_nday,Box_nout],layout = item_layout)\n",
    "input_widgets_row5 = widgets.HBox([button_model,button_fit])\n",
    "\n",
    "tab_dataset = widgets.Tab([output_df_st,output_df_train,output_df_val,output_df_test,output_df_col,df_train,df_val,df_test])\n",
    "tab_dataset.set_title(0, 'Dataset station')\n",
    "tab_dataset.set_title(1, 'train-data')\n",
    "tab_dataset.set_title(2, 'validation-data')\n",
    "tab_dataset.set_title(3, 'test-data')\n",
    "tab_dataset.set_title(4, 'Drop-data')\n",
    "tab_dataset.set_title(5, 'Dataset-train')\n",
    "tab_dataset.set_title(6, 'Dataset-validation')\n",
    "tab_dataset.set_title(7, 'Dataset-test')\n",
    "dashboard_dataset = widgets.VBox([input_widgets_row1,input_widgets_row2])\n",
    "display(dashboard_dataset)\n",
    "display(tab_dataset)\n",
    "\n",
    "dashboard_model = widgets.VBox([input_widgets_row3,input_widgets_row4,input_widgets_row5])\n",
    "tab_model = widgets.Tab([output_create_model,output_fit_model])\n",
    "tab_model.set_title(0,'model.summary')\n",
    "tab_model.set_title(1,'model_fit')\n",
    "display(dashboard_model)\n",
    "display(tab_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"parameter creat model : Epochs : %d,  batch_size : %d, n-day : %d, n-out : %d\" % (Slider_Epochs.value, Slider_batch_size.value, Box_nday.value, Box_nout.value))\n",
    "print(\"parameter Model Fit : Units : %d,  activation : %s,\" % (Box_Units.value, dropdown_activation.value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X, test_Y = data_preprocess(frames_test,Box_nday.value,Box_nout.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start = '2019-01-01'\n",
    "date_stop = '2019-01-31'\n",
    "\n",
    "# frames_test.loc[date_start:date_stop]\n",
    "# test_X, test_Y = data_preprocess(frames_test,Box_nday.value,Box_nout.value)\n",
    "n_day = Box_nday.value\n",
    "n_out = Box_nout.value\n",
    "values_df = frames_test.loc[date_start:date_stop].values    #ตัด header กับ idx ออก เป็น array matrix\n",
    "n_features = frames_test.loc[date_start:date_stop].shape[1]\n",
    "# print(n_features)\n",
    "\n",
    "# ensure all data is float\n",
    "values = values_df.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, n_day, n_out)\n",
    "# print(reframed.shape)\n",
    "# print(reframed.head())\n",
    "\n",
    "# datasets\n",
    "values = reframed.values\n",
    "\n",
    "#input \n",
    "n_obs = n_day * n_features\n",
    "dataset_X, dataset_y = values[:, :n_obs], values[:, -1]\n",
    "# print(dataset_X.shape, len(dataset_X), dataset_y.shape) \n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "dataset_X = dataset_X.reshape((dataset_X.shape[0], n_day, n_features))\n",
    "# print(dataset_X.shape, dataset_y.shape)\n",
    "\n",
    "# # make a prediction\n",
    "yhat = model_out.predict(dataset_X)\n",
    "test_X_reshape = dataset_X.reshape((dataset_X.shape[0], n_day*n_features))\n",
    "    \n",
    "# invert scaling for forecast\n",
    "# inv_yhat = concatenate((yhat, test_X[:, -29:]), axis=1)\n",
    "inv_yhat = concatenate((test_X_reshape[:, :(n_features-1)], yhat), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,-1]\n",
    "\n",
    "# invert scaling for actual\n",
    "test_y_reshape = dataset_y.reshape((len(dataset_y), 1))\n",
    "# inv_y = concatenate((test_y, test_X[:, -29:]), axis=1)\n",
    "inv_y = concatenate((test_X_reshape[:, :(n_features-1)], test_y_reshape), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,-1]\n",
    "# # calculate RMSE\n",
    "# rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "# print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "forecast_data = (abs(inv_yhat[:])) \n",
    "label_data =  inv_y[:]\n",
    "\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(label_data,forecast_data))\n",
    "\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "df = frames_test.loc[date_start:date_stop].reset_index()\n",
    "date_time_predict = pd.to_datetime(df.pop('date'))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "def plot_Perfor():\n",
    "      plt.figure()\n",
    "      plt.plot(date_time_predict[n_day+n_out-1:],label_data,label='label',marker='.')\n",
    "      plt.plot(date_time_predict[n_day+n_out-1:],forecast_data,label='forecast',marker='.')\n",
    "\n",
    "      plt.ylabel('BPH volume')\n",
    "      plt.xlabel('Date time')\n",
    "      # plt.title(file_name +'  RMSE: %.3f' % rmse)\n",
    "      plt.legend()\n",
    "      plt.grid(True)\n",
    "      # plt.show()\n",
    "\n",
    "plot_Perfor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_time_predict\n",
    "date_time_predict[n_day+n_out-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = frames_test['bph'].loc[date_start:date_stop]\n",
    "df_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_plot = 'data test'\n",
    "cols = ['mirid bug', 'bph']\n",
    "date_start = '2015-01-01'\n",
    "date_stop = '2017-12-31'\n",
    "plot_data(frames_train,cols,name_plot,date_start,date_stop)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = data_preprocess(frames_train,7,3)\n",
    "val_X,val_y = data_preprocess(frames_val,7,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "Epoch 1/100\n",
      "9/9 - 0s - loss: 0.0255 - accuracy: 0.0736 - val_loss: 0.0684 - val_accuracy: 0.0225 - 249ms/epoch - 28ms/step\n",
      "Epoch 2/100\n",
      "9/9 - 0s - loss: 0.0257 - accuracy: 0.0736 - val_loss: 0.0693 - val_accuracy: 0.0225 - 202ms/epoch - 22ms/step\n",
      "Epoch 3/100\n",
      "9/9 - 0s - loss: 0.0261 - accuracy: 0.0736 - val_loss: 0.0645 - val_accuracy: 0.0225 - 227ms/epoch - 25ms/step\n",
      "Epoch 4/100\n",
      "9/9 - 0s - loss: 0.0258 - accuracy: 0.0736 - val_loss: 0.0635 - val_accuracy: 0.0225 - 219ms/epoch - 24ms/step\n",
      "Epoch 5/100\n",
      "9/9 - 0s - loss: 0.0263 - accuracy: 0.0736 - val_loss: 0.0676 - val_accuracy: 0.0225 - 219ms/epoch - 24ms/step\n",
      "Epoch 6/100\n",
      "9/9 - 0s - loss: 0.0257 - accuracy: 0.0736 - val_loss: 0.0690 - val_accuracy: 0.0225 - 223ms/epoch - 25ms/step\n",
      "Epoch 7/100\n",
      "9/9 - 0s - loss: 0.0265 - accuracy: 0.0736 - val_loss: 0.0629 - val_accuracy: 0.0225 - 200ms/epoch - 22ms/step\n",
      "Epoch 8/100\n",
      "9/9 - 0s - loss: 0.0256 - accuracy: 0.0736 - val_loss: 0.0640 - val_accuracy: 0.0225 - 207ms/epoch - 23ms/step\n",
      "Epoch 9/100\n",
      "9/9 - 0s - loss: 0.0253 - accuracy: 0.0736 - val_loss: 0.0681 - val_accuracy: 0.0225 - 194ms/epoch - 22ms/step\n",
      "Epoch 10/100\n",
      "9/9 - 0s - loss: 0.0253 - accuracy: 0.0736 - val_loss: 0.0666 - val_accuracy: 0.0225 - 185ms/epoch - 21ms/step\n",
      "Epoch 11/100\n",
      "9/9 - 0s - loss: 0.0248 - accuracy: 0.0736 - val_loss: 0.0636 - val_accuracy: 0.0225 - 191ms/epoch - 21ms/step\n",
      "Epoch 12/100\n",
      "9/9 - 0s - loss: 0.0246 - accuracy: 0.0736 - val_loss: 0.0639 - val_accuracy: 0.0225 - 198ms/epoch - 22ms/step\n",
      "Epoch 13/100\n",
      "9/9 - 0s - loss: 0.0246 - accuracy: 0.0736 - val_loss: 0.0666 - val_accuracy: 0.0225 - 194ms/epoch - 22ms/step\n",
      "Epoch 14/100\n",
      "9/9 - 0s - loss: 0.0249 - accuracy: 0.0736 - val_loss: 0.0675 - val_accuracy: 0.0225 - 190ms/epoch - 21ms/step\n",
      "Epoch 15/100\n",
      "9/9 - 0s - loss: 0.0254 - accuracy: 0.0736 - val_loss: 0.0624 - val_accuracy: 0.0225 - 187ms/epoch - 21ms/step\n",
      "Epoch 16/100\n",
      "9/9 - 0s - loss: 0.0247 - accuracy: 0.0736 - val_loss: 0.0626 - val_accuracy: 0.0225 - 174ms/epoch - 19ms/step\n",
      "Epoch 17/100\n",
      "9/9 - 0s - loss: 0.0250 - accuracy: 0.0736 - val_loss: 0.0661 - val_accuracy: 0.0225 - 176ms/epoch - 20ms/step\n",
      "Epoch 18/100\n",
      "9/9 - 0s - loss: 0.0247 - accuracy: 0.0736 - val_loss: 0.0666 - val_accuracy: 0.0225 - 183ms/epoch - 20ms/step\n",
      "Epoch 19/100\n",
      "9/9 - 0s - loss: 0.0250 - accuracy: 0.0736 - val_loss: 0.0623 - val_accuracy: 0.0225 - 177ms/epoch - 20ms/step\n",
      "Epoch 20/100\n",
      "9/9 - 0s - loss: 0.0241 - accuracy: 0.0736 - val_loss: 0.0621 - val_accuracy: 0.0225 - 177ms/epoch - 20ms/step\n",
      "Epoch 21/100\n",
      "9/9 - 0s - loss: 0.0245 - accuracy: 0.0736 - val_loss: 0.0656 - val_accuracy: 0.0225 - 178ms/epoch - 20ms/step\n",
      "Epoch 22/100\n",
      "9/9 - 0s - loss: 0.0243 - accuracy: 0.0736 - val_loss: 0.0664 - val_accuracy: 0.0225 - 183ms/epoch - 20ms/step\n",
      "Epoch 23/100\n",
      "9/9 - 0s - loss: 0.0247 - accuracy: 0.0736 - val_loss: 0.0614 - val_accuracy: 0.0225 - 174ms/epoch - 19ms/step\n",
      "Epoch 24/100\n",
      "9/9 - 0s - loss: 0.0238 - accuracy: 0.0736 - val_loss: 0.0616 - val_accuracy: 0.0225 - 181ms/epoch - 20ms/step\n",
      "Epoch 25/100\n",
      "9/9 - 0s - loss: 0.0241 - accuracy: 0.0736 - val_loss: 0.0659 - val_accuracy: 0.0225 - 171ms/epoch - 19ms/step\n",
      "Epoch 26/100\n",
      "9/9 - 0s - loss: 0.0240 - accuracy: 0.0736 - val_loss: 0.0652 - val_accuracy: 0.0225 - 209ms/epoch - 23ms/step\n",
      "Epoch 27/100\n",
      "9/9 - 0s - loss: 0.0239 - accuracy: 0.0736 - val_loss: 0.0611 - val_accuracy: 0.0225 - 197ms/epoch - 22ms/step\n",
      "Epoch 28/100\n",
      "9/9 - 0s - loss: 0.0234 - accuracy: 0.0736 - val_loss: 0.0615 - val_accuracy: 0.0225 - 205ms/epoch - 23ms/step\n",
      "Epoch 29/100\n",
      "9/9 - 0s - loss: 0.0234 - accuracy: 0.0736 - val_loss: 0.0653 - val_accuracy: 0.0225 - 184ms/epoch - 20ms/step\n",
      "Epoch 30/100\n",
      "9/9 - 0s - loss: 0.0237 - accuracy: 0.0736 - val_loss: 0.0649 - val_accuracy: 0.0225 - 192ms/epoch - 21ms/step\n",
      "Epoch 31/100\n",
      "9/9 - 0s - loss: 0.0235 - accuracy: 0.0736 - val_loss: 0.0606 - val_accuracy: 0.0225 - 195ms/epoch - 22ms/step\n",
      "Epoch 32/100\n",
      "9/9 - 0s - loss: 0.0230 - accuracy: 0.0736 - val_loss: 0.0610 - val_accuracy: 0.0225 - 233ms/epoch - 26ms/step\n",
      "Epoch 33/100\n",
      "9/9 - 0s - loss: 0.0231 - accuracy: 0.0736 - val_loss: 0.0648 - val_accuracy: 0.0225 - 199ms/epoch - 22ms/step\n",
      "Epoch 34/100\n",
      "9/9 - 0s - loss: 0.0233 - accuracy: 0.0736 - val_loss: 0.0644 - val_accuracy: 0.0225 - 229ms/epoch - 25ms/step\n",
      "Epoch 35/100\n",
      "9/9 - 0s - loss: 0.0231 - accuracy: 0.0736 - val_loss: 0.0606 - val_accuracy: 0.0225 - 205ms/epoch - 23ms/step\n",
      "Epoch 36/100\n",
      "9/9 - 0s - loss: 0.0226 - accuracy: 0.0736 - val_loss: 0.0604 - val_accuracy: 0.0225 - 202ms/epoch - 22ms/step\n",
      "Epoch 37/100\n",
      "9/9 - 0s - loss: 0.0228 - accuracy: 0.0736 - val_loss: 0.0637 - val_accuracy: 0.0225 - 211ms/epoch - 23ms/step\n",
      "Epoch 38/100\n",
      "9/9 - 0s - loss: 0.0229 - accuracy: 0.0736 - val_loss: 0.0647 - val_accuracy: 0.0225 - 197ms/epoch - 22ms/step\n",
      "Epoch 39/100\n",
      "9/9 - 0s - loss: 0.0231 - accuracy: 0.0736 - val_loss: 0.0599 - val_accuracy: 0.0225 - 189ms/epoch - 21ms/step\n",
      "Epoch 40/100\n",
      "9/9 - 0s - loss: 0.0223 - accuracy: 0.0736 - val_loss: 0.0604 - val_accuracy: 0.0225 - 182ms/epoch - 20ms/step\n",
      "Epoch 41/100\n",
      "9/9 - 0s - loss: 0.0224 - accuracy: 0.0736 - val_loss: 0.0644 - val_accuracy: 0.0225 - 178ms/epoch - 20ms/step\n",
      "Epoch 42/100\n",
      "9/9 - 0s - loss: 0.0224 - accuracy: 0.0736 - val_loss: 0.0641 - val_accuracy: 0.0225 - 182ms/epoch - 20ms/step\n",
      "Epoch 43/100\n",
      "9/9 - 0s - loss: 0.0226 - accuracy: 0.0736 - val_loss: 0.0607 - val_accuracy: 0.0225 - 192ms/epoch - 21ms/step\n",
      "Epoch 44/100\n",
      "9/9 - 0s - loss: 0.0216 - accuracy: 0.0736 - val_loss: 0.0605 - val_accuracy: 0.0225 - 174ms/epoch - 19ms/step\n",
      "Epoch 45/100\n",
      "9/9 - 0s - loss: 0.0223 - accuracy: 0.0736 - val_loss: 0.0629 - val_accuracy: 0.0225 - 188ms/epoch - 21ms/step\n",
      "Epoch 46/100\n",
      "9/9 - 0s - loss: 0.0218 - accuracy: 0.0736 - val_loss: 0.0644 - val_accuracy: 0.0225 - 186ms/epoch - 21ms/step\n",
      "Epoch 47/100\n",
      "9/9 - 0s - loss: 0.0227 - accuracy: 0.0736 - val_loss: 0.0607 - val_accuracy: 0.0225 - 187ms/epoch - 21ms/step\n",
      "Epoch 48/100\n",
      "9/9 - 0s - loss: 0.0213 - accuracy: 0.0736 - val_loss: 0.0606 - val_accuracy: 0.0225 - 188ms/epoch - 21ms/step\n",
      "Epoch 49/100\n",
      "9/9 - 0s - loss: 0.0218 - accuracy: 0.0736 - val_loss: 0.0627 - val_accuracy: 0.0225 - 183ms/epoch - 20ms/step\n",
      "Epoch 50/100\n",
      "9/9 - 0s - loss: 0.0214 - accuracy: 0.0736 - val_loss: 0.0644 - val_accuracy: 0.0225 - 209ms/epoch - 23ms/step\n",
      "Epoch 51/100\n",
      "9/9 - 0s - loss: 0.0222 - accuracy: 0.0736 - val_loss: 0.0612 - val_accuracy: 0.0225 - 187ms/epoch - 21ms/step\n",
      "Epoch 52/100\n",
      "9/9 - 0s - loss: 0.0209 - accuracy: 0.0736 - val_loss: 0.0606 - val_accuracy: 0.0225 - 194ms/epoch - 22ms/step\n",
      "Epoch 53/100\n",
      "9/9 - 0s - loss: 0.0215 - accuracy: 0.0736 - val_loss: 0.0620 - val_accuracy: 0.0225 - 187ms/epoch - 21ms/step\n",
      "Epoch 54/100\n",
      "9/9 - 0s - loss: 0.0211 - accuracy: 0.0736 - val_loss: 0.0648 - val_accuracy: 0.0225 - 193ms/epoch - 21ms/step\n",
      "Epoch 55/100\n",
      "9/9 - 0s - loss: 0.0222 - accuracy: 0.0736 - val_loss: 0.0623 - val_accuracy: 0.0225 - 194ms/epoch - 22ms/step\n",
      "Epoch 56/100\n",
      "9/9 - 0s - loss: 0.0207 - accuracy: 0.0736 - val_loss: 0.0610 - val_accuracy: 0.0225 - 196ms/epoch - 22ms/step\n",
      "Epoch 57/100\n",
      "9/9 - 0s - loss: 0.0208 - accuracy: 0.0736 - val_loss: 0.0609 - val_accuracy: 0.0225 - 181ms/epoch - 20ms/step\n",
      "Epoch 58/100\n",
      "9/9 - 0s - loss: 0.0211 - accuracy: 0.0736 - val_loss: 0.0637 - val_accuracy: 0.0225 - 194ms/epoch - 22ms/step\n",
      "Epoch 59/100\n",
      "9/9 - 0s - loss: 0.0218 - accuracy: 0.0736 - val_loss: 0.0652 - val_accuracy: 0.0225 - 186ms/epoch - 21ms/step\n",
      "Epoch 60/100\n",
      "9/9 - 0s - loss: 0.0222 - accuracy: 0.0736 - val_loss: 0.0600 - val_accuracy: 0.0225 - 185ms/epoch - 21ms/step\n",
      "Epoch 61/100\n",
      "9/9 - 0s - loss: 0.0216 - accuracy: 0.0736 - val_loss: 0.0599 - val_accuracy: 0.0225 - 196ms/epoch - 22ms/step\n",
      "Epoch 62/100\n",
      "9/9 - 0s - loss: 0.0215 - accuracy: 0.0736 - val_loss: 0.0654 - val_accuracy: 0.0225 - 219ms/epoch - 24ms/step\n",
      "Epoch 63/100\n",
      "9/9 - 0s - loss: 0.0216 - accuracy: 0.0736 - val_loss: 0.0649 - val_accuracy: 0.0225 - 235ms/epoch - 26ms/step\n",
      "Epoch 64/100\n",
      "9/9 - 0s - loss: 0.0212 - accuracy: 0.0736 - val_loss: 0.0607 - val_accuracy: 0.0225 - 250ms/epoch - 28ms/step\n",
      "Epoch 65/100\n",
      "9/9 - 0s - loss: 0.0208 - accuracy: 0.0736 - val_loss: 0.0613 - val_accuracy: 0.0225 - 209ms/epoch - 23ms/step\n",
      "Epoch 66/100\n",
      "9/9 - 0s - loss: 0.0210 - accuracy: 0.0736 - val_loss: 0.0650 - val_accuracy: 0.0225 - 193ms/epoch - 21ms/step\n",
      "Epoch 67/100\n",
      "9/9 - 0s - loss: 0.0215 - accuracy: 0.0736 - val_loss: 0.0659 - val_accuracy: 0.0225 - 196ms/epoch - 22ms/step\n",
      "Epoch 68/100\n",
      "9/9 - 0s - loss: 0.0218 - accuracy: 0.0736 - val_loss: 0.0611 - val_accuracy: 0.0225 - 190ms/epoch - 21ms/step\n",
      "Epoch 69/100\n",
      "9/9 - 0s - loss: 0.0210 - accuracy: 0.0736 - val_loss: 0.0610 - val_accuracy: 0.0225 - 193ms/epoch - 21ms/step\n",
      "Epoch 70/100\n",
      "9/9 - 0s - loss: 0.0211 - accuracy: 0.0736 - val_loss: 0.0662 - val_accuracy: 0.0225 - 194ms/epoch - 22ms/step\n",
      "Epoch 71/100\n",
      "9/9 - 0s - loss: 0.0212 - accuracy: 0.0736 - val_loss: 0.0658 - val_accuracy: 0.0225 - 188ms/epoch - 21ms/step\n",
      "Epoch 72/100\n",
      "9/9 - 0s - loss: 0.0208 - accuracy: 0.0736 - val_loss: 0.0613 - val_accuracy: 0.0225 - 202ms/epoch - 22ms/step\n",
      "Epoch 73/100\n",
      "9/9 - 0s - loss: 0.0206 - accuracy: 0.0736 - val_loss: 0.0625 - val_accuracy: 0.0225 - 217ms/epoch - 24ms/step\n",
      "Epoch 74/100\n",
      "9/9 - 0s - loss: 0.0206 - accuracy: 0.0736 - val_loss: 0.0665 - val_accuracy: 0.0225 - 194ms/epoch - 22ms/step\n",
      "Epoch 75/100\n",
      "9/9 - 0s - loss: 0.0213 - accuracy: 0.0736 - val_loss: 0.0660 - val_accuracy: 0.0225 - 198ms/epoch - 22ms/step\n",
      "Epoch 76/100\n",
      "9/9 - 0s - loss: 0.0207 - accuracy: 0.0736 - val_loss: 0.0616 - val_accuracy: 0.0225 - 201ms/epoch - 22ms/step\n",
      "Epoch 77/100\n",
      "9/9 - 0s - loss: 0.0207 - accuracy: 0.0736 - val_loss: 0.0623 - val_accuracy: 0.0225 - 191ms/epoch - 21ms/step\n",
      "Epoch 78/100\n",
      "9/9 - 0s - loss: 0.0207 - accuracy: 0.0736 - val_loss: 0.0677 - val_accuracy: 0.0225 - 190ms/epoch - 21ms/step\n",
      "Epoch 79/100\n",
      "9/9 - 0s - loss: 0.0213 - accuracy: 0.0736 - val_loss: 0.0655 - val_accuracy: 0.0225 - 183ms/epoch - 20ms/step\n",
      "Epoch 80/100\n",
      "9/9 - 0s - loss: 0.0202 - accuracy: 0.0736 - val_loss: 0.0622 - val_accuracy: 0.0225 - 185ms/epoch - 21ms/step\n",
      "Epoch 81/100\n",
      "9/9 - 0s - loss: 0.0203 - accuracy: 0.0736 - val_loss: 0.0635 - val_accuracy: 0.0225 - 199ms/epoch - 22ms/step\n",
      "Epoch 82/100\n",
      "9/9 - 0s - loss: 0.0203 - accuracy: 0.0736 - val_loss: 0.0669 - val_accuracy: 0.0225 - 195ms/epoch - 22ms/step\n",
      "Epoch 83/100\n",
      "9/9 - 0s - loss: 0.0208 - accuracy: 0.0736 - val_loss: 0.0673 - val_accuracy: 0.0225 - 194ms/epoch - 22ms/step\n",
      "Epoch 84/100\n",
      "9/9 - 0s - loss: 0.0209 - accuracy: 0.0736 - val_loss: 0.0629 - val_accuracy: 0.0225 - 203ms/epoch - 23ms/step\n",
      "Epoch 85/100\n",
      "9/9 - 0s - loss: 0.0205 - accuracy: 0.0736 - val_loss: 0.0622 - val_accuracy: 0.0225 - 187ms/epoch - 21ms/step\n",
      "Epoch 86/100\n",
      "9/9 - 0s - loss: 0.0209 - accuracy: 0.0736 - val_loss: 0.0673 - val_accuracy: 0.0225 - 200ms/epoch - 22ms/step\n",
      "Epoch 87/100\n",
      "9/9 - 0s - loss: 0.0208 - accuracy: 0.0736 - val_loss: 0.0682 - val_accuracy: 0.0225 - 185ms/epoch - 21ms/step\n",
      "Epoch 88/100\n",
      "9/9 - 0s - loss: 0.0210 - accuracy: 0.0736 - val_loss: 0.0632 - val_accuracy: 0.0225 - 192ms/epoch - 21ms/step\n",
      "Epoch 89/100\n",
      "9/9 - 0s - loss: 0.0204 - accuracy: 0.0736 - val_loss: 0.0633 - val_accuracy: 0.0225 - 190ms/epoch - 21ms/step\n",
      "Epoch 90/100\n",
      "9/9 - 0s - loss: 0.0204 - accuracy: 0.0736 - val_loss: 0.0682 - val_accuracy: 0.0225 - 187ms/epoch - 21ms/step\n",
      "Epoch 91/100\n",
      "9/9 - 0s - loss: 0.0204 - accuracy: 0.0736 - val_loss: 0.0678 - val_accuracy: 0.0225 - 191ms/epoch - 21ms/step\n",
      "Epoch 92/100\n",
      "9/9 - 0s - loss: 0.0201 - accuracy: 0.0736 - val_loss: 0.0645 - val_accuracy: 0.0225 - 195ms/epoch - 22ms/step\n",
      "Epoch 93/100\n",
      "9/9 - 0s - loss: 0.0197 - accuracy: 0.0736 - val_loss: 0.0646 - val_accuracy: 0.0225 - 187ms/epoch - 21ms/step\n",
      "Epoch 94/100\n",
      "9/9 - 0s - loss: 0.0203 - accuracy: 0.0736 - val_loss: 0.0680 - val_accuracy: 0.0225 - 189ms/epoch - 21ms/step\n",
      "Epoch 95/100\n",
      "9/9 - 0s - loss: 0.0207 - accuracy: 0.0736 - val_loss: 0.0687 - val_accuracy: 0.0225 - 185ms/epoch - 21ms/step\n",
      "Epoch 96/100\n",
      "9/9 - 0s - loss: 0.0209 - accuracy: 0.0736 - val_loss: 0.0654 - val_accuracy: 0.0225 - 203ms/epoch - 23ms/step\n",
      "Epoch 97/100\n",
      "9/9 - 0s - loss: 0.0197 - accuracy: 0.0736 - val_loss: 0.0644 - val_accuracy: 0.0225 - 215ms/epoch - 24ms/step\n",
      "Epoch 98/100\n",
      "9/9 - 0s - loss: 0.0202 - accuracy: 0.0736 - val_loss: 0.0681 - val_accuracy: 0.0225 - 209ms/epoch - 23ms/step\n",
      "Epoch 99/100\n",
      "9/9 - 0s - loss: 0.0200 - accuracy: 0.0736 - val_loss: 0.0695 - val_accuracy: 0.0225 - 208ms/epoch - 23ms/step\n",
      "Epoch 100/100\n",
      "9/9 - 0s - loss: 0.0210 - accuracy: 0.0736 - val_loss: 0.0654 - val_accuracy: 0.0225 - 203ms/epoch - 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17248), started 16 days, 15:54:30 ago. (Use '!kill 17248' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c4cdfe9f06d92b55\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c4cdfe9f06d92b55\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# fit network\n",
    "Epochs = 100\n",
    "batch_size = 128\n",
    "history = model_out.fit(train_X, train_Y, \n",
    "                    epochs=Epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=(val_X, val_Y), \n",
    "                    verbose=2, \n",
    "                    # callbacks=[cp_callback,es_callback], \n",
    "                    # callbacks=[cp_callback],\n",
    "                    callbacks=[tensorboard_callback],\n",
    "                    shuffle=False)\n",
    "\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, acc = model.evaluate(val_X, val_y, verbose=2)\n",
    "var_loss = round(loss,5)\n",
    "print('Accuracy : ', acc)\n",
    "print('var_loss is : ', var_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf_bph')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd774bab908e2692cca9e79cb5c72f2d9e30983f1ff0ef40e6e4a8307d564387"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
