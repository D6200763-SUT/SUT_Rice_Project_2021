{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.4\n",
      "1.22.3\n",
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "# import \n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "# import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import platform\n",
    "print(platform.python_version())\n",
    "print(np.__version__)\n",
    "print(tf.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All low RiceCenter 34 station\n"
     ]
    }
   ],
   "source": [
    "# File name and Path\n",
    "cwd = os.getcwd()\n",
    "path_adress = \"/Import_Dataset/\"\n",
    "\n",
    "# ข้อมูล พิกัดสถานที่เเละรายชื้ออ้างอิงสถานที่ตรวจวัด\n",
    "file_name_st = 'Data_lat_long_Rice research Center'\n",
    "csv_file_st = cwd + path_adress + file_name_st + '.csv'\n",
    "\n",
    "df_st = pd.read_csv(csv_file_st)\n",
    "print(\"All low RiceCenter {} station\" .format(df_st.shape[0]))\n",
    "\n",
    "data_list_st_num = list(range(0, len(df_st)))\n",
    "data_list_st_name = []\n",
    "data_list_st_name = df_st['nameEng'].values.tolist()\n",
    "data_list_st_lat = df_st['Latitude'].values.tolist()\n",
    "data_list_st_long = df_st['Longitude'].values.tolist()\n",
    "\n",
    "#################-- Moving Averag data --#################\n",
    "def mavr_dataset(frames_sma,rolling_num,our_rate):    \n",
    "    h_name = list(frames_sma)\n",
    "    # frames_sma['mirid bug'] = frames_sma['mirid bug'].rolling(rolling_num, min_periods=1).sum().round(1)\n",
    "    # frames_sma['bph'] = frames_sma['bph'].rolling(rolling_num, min_periods=1).sum().round(1)\n",
    "    frames_sma['mirid bug'] = frames_sma['mirid bug'].rolling(rolling_num, min_periods=1).mean().round(1)\n",
    "    frames_sma['bph'] = frames_sma['bph'].rolling(rolling_num, min_periods=1).mean().round(1)\n",
    "    for i in range(7,14):\n",
    "        frames_sma[h_name[i]] = frames_sma[h_name[i]].rolling(rolling_num, min_periods=1).mean().round(1)\n",
    "\n",
    "    constant_subset = frames_sma[::our_rate] \n",
    "    return constant_subset\n",
    "\n",
    "#################-- creat_dataset  --#################\n",
    "def creat_dataset(st_BPH = 'ALL', y_1='2015', y_2='2019', m_avr='ALL-DAY'):\n",
    "    for i in range(len(data_list_st_num)):\n",
    "        st = i\n",
    "        file_name = df_st['nameEng'][data_list_st_num[st]]\n",
    "        name_input = file_name\n",
    "        name_locals = 'stN_' + file_name\n",
    "\n",
    "        # File name and Path\n",
    "        csv_file = cwd + path_adress + name_input + '.csv'\n",
    "        # index_col=0 , index_col=None\n",
    "        dataset = pd.read_csv(csv_file, header=0,\n",
    "                              index_col=0, encoding=\"TIS-620\")\n",
    "\n",
    "        if m_avr == '3-DAY':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,3,3)\n",
    "        elif m_avr == '7-DAY':\n",
    "            locals()[name_locals] = mavr_dataset(dataset,7,7)\n",
    "        else :\n",
    "            locals()[name_locals] = mavr_dataset(dataset,7,1)\n",
    "            # print(f'Dataframe name_station: {st+1 , name_input}')\n",
    "        print('wait......')\n",
    "        del dataset\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    date_start = y_1 + '-01' + '-01'\n",
    "    date_stop = y_2 + '-12' + '-31'\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "    if st_BPH != 'ALL' :\n",
    "        # file_name = df_st['nameEng'][data_list_st_num[st_BPH]]\n",
    "        file_name = st_BPH\n",
    "        locals_input = 'stN_' + file_name\n",
    "        print(locals_input)\n",
    "        dataset_st = locals()[locals_input].loc[date_start:date_stop]\n",
    "        frames_st = dataset_st\n",
    "    else:\n",
    "        m = 0\n",
    "        for j in range(len(data_list_st_num)):\n",
    "            # for j in range(df_st.shape[0]):\n",
    "            # file_name = df_st['nameEng'][j]\n",
    "            file_name = df_st['nameEng'][data_list_st_num[j]]\n",
    "            locals_input = 'stN_' + file_name\n",
    "            print(locals_input)\n",
    "            dataset_st = locals()[locals_input].loc[date_start:date_stop]\n",
    "            clear_output(wait=True)\n",
    "            if m == 0:\n",
    "                frames_st = dataset_st\n",
    "                m = m+1\n",
    "                print(m)\n",
    "            else:\n",
    "                frames_st = [frames_st, dataset_st]\n",
    "                frames_st = pd.concat(frames_st)\n",
    "\n",
    "    # dataset    \n",
    "    frames = frames_st\n",
    "    return frames\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#################-- Drop Colum  --#################\n",
    "def drop_col(df_drop,drop = ['address']):\n",
    "    for j in range(len(drop)):\n",
    "        df_drop = df_drop.drop(drop[j], axis=1)\n",
    "    return df_drop\n",
    "\n",
    "#################--      Plot        --#################\n",
    "def plot_data(frames_train,df_name):\n",
    "    #ตรวจสอบข้อมูล dataset \n",
    "    plt.figure()\n",
    "    df_plot = frames_train\n",
    "    df_plot.plot(lw=1,grid=True,figsize=(13,30),subplots=True)\n",
    "    plt.xlabel('Date time-'+ df_name)\n",
    "    plt.legend()\n",
    "    # plt.show()   \n",
    "\n",
    "#################--                  --#################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################-- convert series to supervised learning --#################\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "\n",
    "\n",
    "def data_preprocess(data_frames,n_day,n_out):\n",
    "      values_df = data_frames.values    #ตัด header กับ idx ออก เป็น array matrix\n",
    "      n_features = data_frames.shape[1]\n",
    "      # print(n_features)\n",
    "      \n",
    "      # ensure all data is float\n",
    "      values = values_df.astype('float32')\n",
    "      # normalize features\n",
    "      scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "      scaled = scaler.fit_transform(values)\n",
    "\n",
    "      # frame as supervised learning\n",
    "      reframed = series_to_supervised(scaled, n_day, n_out)\n",
    "      # print(reframed.shape)\n",
    "      # print(reframed.head())\n",
    "\n",
    "      # datasets\n",
    "      values = reframed.values\n",
    "\n",
    "      #input \n",
    "      n_obs = n_day * n_features\n",
    "      dataset_X, dataset_y = values[:, :n_obs], values[:, -1]\n",
    "      # print(dataset_X.shape, len(dataset_X), dataset_y.shape) \n",
    "\n",
    "      # reshape input to be 3D [samples, timesteps, features]\n",
    "      dataset_X = dataset_X.reshape((dataset_X.shape[0], n_day, n_features))\n",
    "      # print(dataset_X.shape, dataset_y.shape)\n",
    "\n",
    "      return dataset_X,dataset_y\n",
    "\n",
    "def create_model(unit,shape_1,shape_2,activ = 'relu',optim = 0.0001,loss_input = 'mae',metrics_input = 'accuracy'):\n",
    "      model = tf.keras.models.Sequential([\n",
    "                  # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "                  keras.layers.LSTM(units=unit, input_shape=(shape_1, shape_2),activation=activ),\n",
    "                  keras.layers.Dense(units=1)\n",
    "            ])            \n",
    "      Optimizer = tf.keras.optimizers.Adam(optim)\n",
    "      model.compile(Optimizer, loss=loss_input, metrics= metrics_input)\n",
    "      # model.summary()\n",
    "      return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dfdb6d1d40649e69e85c9044fe7c16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='station :', options=('ALL', 'Chachoengsao Rice research Ce…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193eacd4c4164675bba51b12d1c53893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output()), _titles={'0': '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#################-- Dashboard --#################\n",
    "\n",
    "\n",
    "ALL = 'ALL'\n",
    "def unique_sorted_values_plus_ALL(array):\n",
    "    unique = array.unique().tolist()\n",
    "    unique.sort()\n",
    "    unique.insert(0, ALL)\n",
    "    return unique\n",
    "\n",
    "#################-- DataFrame station --#################\n",
    "output_df_st= widgets.Output()\n",
    "def dropdown_station_eventhandler(change):\n",
    "    output_df_st.clear_output()\n",
    "    with output_df_st:  \n",
    "        if (change.new == ALL):\n",
    "            display(df_st)\n",
    "        else:\n",
    "            display(df_st[df_st.nameEng == change.new])\n",
    "\n",
    "#################-- select sampling --#################\n",
    "def dropdown_sampling_eventhandler(change):\n",
    "    return dropdown_sampling_data\n",
    "\n",
    "#################-- select year train --#################\n",
    "def dropdown_year_t1_eventhandler(change):\n",
    "    return dropdown_year_train1\n",
    "def dropdown_year_t2_eventhandler(change):\n",
    "    return dropdown_year_train2\n",
    "def dropdown_year_v1_eventhandler(change):\n",
    "    return dropdown_year_val1\n",
    "def dropdown_year_v2_eventhandler(change):\n",
    "    return dropdown_year_val2\n",
    "def dropdown_year_ts1_eventhandler(change):\n",
    "    return dropdown_year_test1\n",
    "def dropdown_year_ts2_eventhandler(change):\n",
    "    return dropdown_year_test2\n",
    "\n",
    "#################-- load data  --#################\n",
    "output_df_train = widgets.Output()\n",
    "output_df_val = widgets.Output()\n",
    "output_df_test = widgets.Output()\n",
    "output_df_col = widgets.Output()\n",
    "def clicked_load(b):\n",
    "   \n",
    "    global df_out_train\n",
    "    global df_out_val\n",
    "    global df_out_test\n",
    "    global selected_data\n",
    "    output_df_train.clear_output()\n",
    "    output_df_val.clear_output()\n",
    "    output_df_test.clear_output()\n",
    "    output_df_col.clear_output()\n",
    "    with output_df_train:\n",
    "        df_out_train = creat_dataset(dropdown_name_st.value,dropdown_year_train1.value,dropdown_year_train2.value,dropdown_sampling_data.value)\n",
    "        display(df_out_train)\n",
    "        # display(creat_dataset(dropdown_name_st.value,dropdown_year_train1.value,dropdown_year_train2.value,dropdown_sampling_data.value))\n",
    "    with output_df_val:\n",
    "        df_out_val = creat_dataset(dropdown_name_st.value,dropdown_year_val1.value,dropdown_year_val2.value,dropdown_sampling_data.value)\n",
    "        display(df_out_val)\n",
    "        # display(creat_dataset(dropdown_name_st.value,dropdown_year_val1.value,dropdown_year_val2.value,dropdown_sampling_data.value))\n",
    "    with output_df_test:\n",
    "        df_out_test = creat_dataset(dropdown_name_st.value,dropdown_year_test1.value,dropdown_year_test2.value,dropdown_sampling_data.value)\n",
    "        display(df_out_test)\n",
    "        # display(creat_dataset(dropdown_name_st.value,dropdown_year_test1.value,dropdown_year_test2.value,dropdown_sampling_data.value))\n",
    "    with output_df_col:\n",
    "        checkbox_objects = []\n",
    "        # global selected_data\n",
    "        data = set(chek_list(df_out_train))    \n",
    "        names = []\n",
    "        for key in data:\n",
    "                checkbox_objects.append(widgets.Checkbox(value=False, description=key))\n",
    "                names.append(key)\n",
    " \n",
    "        arg_dict = {names[i]: checkbox for i, checkbox in enumerate(checkbox_objects)}\n",
    "        ui = widgets.HBox(children=checkbox_objects)\n",
    "        display(ui)\n",
    "             \n",
    "        selected_data = []\n",
    "        def select_data(**kwargs):\n",
    "            selected_data.clear()\n",
    "            for key in kwargs:\n",
    "                if kwargs[key] is True:\n",
    "                    selected_data.append(key)\n",
    "            print(selected_data)\n",
    "\n",
    "        widgets.interactive_output(select_data, arg_dict)\n",
    "    \n",
    "    return df_out_train,df_out_val,df_out_test\n",
    "\n",
    "def chek_list(df_ch):\n",
    "        data_col = set(df_ch.columns)    \n",
    "        return data_col\n",
    "\n",
    "#################-- Export data  --#################\n",
    "df_train = widgets.Output()\n",
    "df_val = widgets.Output()\n",
    "df_test = widgets.Output()\n",
    "def clicked_export(b):\n",
    "    df_train.clear_output()\n",
    "    df_val.clear_output()\n",
    "    df_test.clear_output()\n",
    "    global frames_train\n",
    "    global frames_val\n",
    "    global frames_test\n",
    "    with df_train:\n",
    "        frames_train = drop_col(df_out_train,selected_data) \n",
    "        display(frames_train)\n",
    "    with df_val:\n",
    "        frames_val = drop_col(df_out_val,selected_data) \n",
    "        display(frames_val)  \n",
    "    with df_test:\n",
    "        frames_test = drop_col(df_out_test,selected_data) \n",
    "        display(frames_test)     \n",
    "\n",
    "\n",
    "dropdown_name_st = widgets.Dropdown(options = unique_sorted_values_plus_ALL(df_st.nameEng),description='station :')\n",
    "dropdown_sampling_data = widgets.Dropdown(options = ['All-DAY','3-DAY','7-DAY'],valure = 'All-DAY',description='Sampling :')\n",
    "dropdown_year_train1 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],description='Year-train :')\n",
    "dropdown_year_train2 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2017')\n",
    "dropdown_year_val1 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2018',description='Year-val :')\n",
    "dropdown_year_val2 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2018')\n",
    "dropdown_year_test1 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2019',description='Year-test :')\n",
    "dropdown_year_test2 = widgets.Dropdown(options = ['2015','2016','2017','2018','2019','2020'],value = '2019')\n",
    "button_download = widgets.Button(description='load data',disabled=False,button_style='success',tooltip='Click me',icon='check')\n",
    "button_export = widgets.Button(description='export dataset',disabled=False,button_style='success',tooltip='Click me',icon='check')\n",
    "\n",
    "button_download.on_click(clicked_load)\n",
    "button_export.on_click(clicked_export)\n",
    "\n",
    "dropdown_name_st.observe(dropdown_station_eventhandler, names='value')\n",
    "dropdown_sampling_data.observe(dropdown_sampling_eventhandler, names='value')\n",
    "dropdown_year_train1.observe(dropdown_year_t1_eventhandler, names='value')\n",
    "dropdown_year_train2.observe(dropdown_year_t2_eventhandler, names='value')\n",
    "dropdown_year_val1.observe(dropdown_year_v1_eventhandler, names='value')\n",
    "dropdown_year_val2.observe(dropdown_year_v2_eventhandler, names='value')\n",
    "dropdown_year_test1.observe(dropdown_year_ts1_eventhandler, names='value')\n",
    "dropdown_year_test2.observe(dropdown_year_ts2_eventhandler, names='value')\n",
    "\n",
    "item_layout = widgets.Layout(margin='0 0 15px 0')\n",
    "\n",
    "input_widgets_row1 = widgets.HBox([dropdown_name_st,dropdown_year_train1,dropdown_year_train2,dropdown_year_val1,dropdown_year_val2,dropdown_year_test1,dropdown_year_test2],layout = item_layout)\n",
    "input_widgets_row2 = widgets.HBox([dropdown_sampling_data,button_download,button_export],layout = item_layout)\n",
    "\n",
    "\n",
    "tab = widgets.Tab([output_df_st,output_df_train,output_df_val,output_df_test,output_df_col,df_train,df_val,df_test])\n",
    "tab.set_title(0, 'Dataset station')\n",
    "tab.set_title(1, 'train-data')\n",
    "tab.set_title(2, 'validation-data')\n",
    "tab.set_title(3, 'test-data')\n",
    "tab.set_title(4, 'Drop-data')\n",
    "tab.set_title(5, 'Dataset-train')\n",
    "tab.set_title(6, 'Dataset-validation')\n",
    "tab.set_title(7, 'Dataset-test')\n",
    "dashboard = widgets.VBox([input_widgets_row1,input_widgets_row2])\n",
    "display(dashboard)\n",
    "display(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = data_preprocess(frames_train,7,3)\n",
    "val_X,val_y = data_preprocess(frames_val,7,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 19:02:23.830695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-10 19:02:23.838501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-10 19:02:23.838768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-10 19:02:23.839753: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-10 19:02:23.840429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-10 19:02:23.840677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-10 19:02:23.840891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-10 19:02:24.187723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-10 19:02:24.187939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-10 19:02:24.188100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-10 19:02:24.188187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5026 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "model = create_model(100,train_X.shape[1],train_X.shape[2])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "Epoch 1/50\n",
      "292/292 - 2s - loss: 0.0030 - accuracy: 0.4717 - val_loss: 0.0063 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 2/50\n",
      "292/292 - 2s - loss: 0.0026 - accuracy: 0.4717 - val_loss: 0.0052 - val_accuracy: 0.5469 - 2s/epoch - 5ms/step\n",
      "Epoch 3/50\n",
      "292/292 - 2s - loss: 0.0028 - accuracy: 0.4717 - val_loss: 0.0049 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 4/50\n",
      "292/292 - 2s - loss: 0.0029 - accuracy: 0.4717 - val_loss: 0.0066 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 5/50\n",
      "292/292 - 2s - loss: 0.0030 - accuracy: 0.4717 - val_loss: 0.0045 - val_accuracy: 0.5469 - 2s/epoch - 5ms/step\n",
      "Epoch 6/50\n",
      "292/292 - 2s - loss: 0.0027 - accuracy: 0.4717 - val_loss: 0.0055 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 7/50\n",
      "292/292 - 2s - loss: 0.0025 - accuracy: 0.4717 - val_loss: 0.0046 - val_accuracy: 0.5469 - 2s/epoch - 7ms/step\n",
      "Epoch 8/50\n",
      "292/292 - 2s - loss: 0.0026 - accuracy: 0.4717 - val_loss: 0.0058 - val_accuracy: 0.5469 - 2s/epoch - 7ms/step\n",
      "Epoch 9/50\n",
      "292/292 - 2s - loss: 0.0030 - accuracy: 0.4717 - val_loss: 0.0048 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 10/50\n",
      "292/292 - 2s - loss: 0.0027 - accuracy: 0.4717 - val_loss: 0.0051 - val_accuracy: 0.5469 - 2s/epoch - 5ms/step\n",
      "Epoch 11/50\n",
      "292/292 - 2s - loss: 0.0026 - accuracy: 0.4717 - val_loss: 0.0052 - val_accuracy: 0.5469 - 2s/epoch - 5ms/step\n",
      "Epoch 12/50\n",
      "292/292 - 2s - loss: 0.0025 - accuracy: 0.4717 - val_loss: 0.0054 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 13/50\n",
      "292/292 - 2s - loss: 0.0024 - accuracy: 0.4717 - val_loss: 0.0043 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 14/50\n",
      "292/292 - 2s - loss: 0.0024 - accuracy: 0.4717 - val_loss: 0.0046 - val_accuracy: 0.5469 - 2s/epoch - 5ms/step\n",
      "Epoch 15/50\n",
      "292/292 - 2s - loss: 0.0025 - accuracy: 0.4717 - val_loss: 0.0047 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 16/50\n",
      "292/292 - 2s - loss: 0.0029 - accuracy: 0.4717 - val_loss: 0.0052 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 17/50\n",
      "292/292 - 2s - loss: 0.0028 - accuracy: 0.4717 - val_loss: 0.0051 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 18/50\n",
      "292/292 - 2s - loss: 0.0030 - accuracy: 0.4717 - val_loss: 0.0053 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 19/50\n",
      "292/292 - 2s - loss: 0.0027 - accuracy: 0.4717 - val_loss: 0.0068 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 20/50\n",
      "292/292 - 2s - loss: 0.0029 - accuracy: 0.4717 - val_loss: 0.0046 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 21/50\n",
      "292/292 - 2s - loss: 0.0024 - accuracy: 0.4717 - val_loss: 0.0051 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 22/50\n",
      "292/292 - 2s - loss: 0.0027 - accuracy: 0.4717 - val_loss: 0.0053 - val_accuracy: 0.5469 - 2s/epoch - 5ms/step\n",
      "Epoch 23/50\n",
      "292/292 - 2s - loss: 0.0025 - accuracy: 0.4717 - val_loss: 0.0046 - val_accuracy: 0.5469 - 2s/epoch - 5ms/step\n",
      "Epoch 24/50\n",
      "292/292 - 2s - loss: 0.0023 - accuracy: 0.4717 - val_loss: 0.0052 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 25/50\n",
      "292/292 - 2s - loss: 0.0026 - accuracy: 0.4717 - val_loss: 0.0056 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 26/50\n",
      "292/292 - 2s - loss: 0.0023 - accuracy: 0.4717 - val_loss: 0.0052 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 27/50\n",
      "292/292 - 2s - loss: 0.0026 - accuracy: 0.4717 - val_loss: 0.0042 - val_accuracy: 0.5469 - 2s/epoch - 7ms/step\n",
      "Epoch 28/50\n",
      "292/292 - 2s - loss: 0.0024 - accuracy: 0.4717 - val_loss: 0.0043 - val_accuracy: 0.5469 - 2s/epoch - 5ms/step\n",
      "Epoch 29/50\n",
      "292/292 - 2s - loss: 0.0026 - accuracy: 0.4717 - val_loss: 0.0042 - val_accuracy: 0.5469 - 2s/epoch - 7ms/step\n",
      "Epoch 30/50\n",
      "292/292 - 2s - loss: 0.0023 - accuracy: 0.4717 - val_loss: 0.0054 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 31/50\n",
      "292/292 - 2s - loss: 0.0022 - accuracy: 0.4717 - val_loss: 0.0053 - val_accuracy: 0.5469 - 2s/epoch - 5ms/step\n",
      "Epoch 32/50\n",
      "292/292 - 2s - loss: 0.0022 - accuracy: 0.4717 - val_loss: 0.0058 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 33/50\n",
      "292/292 - 2s - loss: 0.0022 - accuracy: 0.4717 - val_loss: 0.0046 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 34/50\n",
      "292/292 - 2s - loss: 0.0023 - accuracy: 0.4717 - val_loss: 0.0042 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 35/50\n",
      "292/292 - 2s - loss: 0.0023 - accuracy: 0.4717 - val_loss: 0.0051 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 36/50\n",
      "292/292 - 2s - loss: 0.0023 - accuracy: 0.4717 - val_loss: 0.0041 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 37/50\n",
      "292/292 - 2s - loss: 0.0021 - accuracy: 0.4717 - val_loss: 0.0053 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 38/50\n",
      "292/292 - 2s - loss: 0.0025 - accuracy: 0.4717 - val_loss: 0.0042 - val_accuracy: 0.5469 - 2s/epoch - 5ms/step\n",
      "Epoch 39/50\n",
      "292/292 - 2s - loss: 0.0023 - accuracy: 0.4717 - val_loss: 0.0053 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 40/50\n",
      "292/292 - 2s - loss: 0.0023 - accuracy: 0.4717 - val_loss: 0.0040 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 41/50\n",
      "292/292 - 2s - loss: 0.0022 - accuracy: 0.4717 - val_loss: 0.0041 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 42/50\n",
      "292/292 - 2s - loss: 0.0023 - accuracy: 0.4717 - val_loss: 0.0042 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 43/50\n",
      "292/292 - 2s - loss: 0.0024 - accuracy: 0.4717 - val_loss: 0.0044 - val_accuracy: 0.5469 - 2s/epoch - 5ms/step\n",
      "Epoch 44/50\n",
      "292/292 - 2s - loss: 0.0023 - accuracy: 0.4717 - val_loss: 0.0039 - val_accuracy: 0.5469 - 2s/epoch - 7ms/step\n",
      "Epoch 45/50\n",
      "292/292 - 2s - loss: 0.0021 - accuracy: 0.4717 - val_loss: 0.0040 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 46/50\n",
      "292/292 - 2s - loss: 0.0020 - accuracy: 0.4717 - val_loss: 0.0042 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 47/50\n",
      "292/292 - 2s - loss: 0.0023 - accuracy: 0.4717 - val_loss: 0.0039 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 48/50\n",
      "292/292 - 2s - loss: 0.0022 - accuracy: 0.4717 - val_loss: 0.0045 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n",
      "Epoch 49/50\n",
      "292/292 - 2s - loss: 0.0020 - accuracy: 0.4717 - val_loss: 0.0042 - val_accuracy: 0.5469 - 2s/epoch - 5ms/step\n",
      "Epoch 50/50\n",
      "292/292 - 2s - loss: 0.0021 - accuracy: 0.4717 - val_loss: 0.0039 - val_accuracy: 0.5469 - 2s/epoch - 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 23876), started 0:04:53 ago. (Use '!kill 23876' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-238bceca7a58f069\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-238bceca7a58f069\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "# fit network\n",
    "Epochs = 50\n",
    "batch_size = 128\n",
    "history = model.fit(train_X, train_y, \n",
    "                    epochs=Epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=(val_X, val_y), \n",
    "                    verbose=2, \n",
    "                    # callbacks=[cp_callback,es_callback], \n",
    "                    # callbacks=[cp_callback],\n",
    "                    callbacks=[tensorboard_callback],\n",
    "                    shuffle=False)\n",
    "\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, acc = model.evaluate(val_X, val_y, verbose=2)\n",
    "var_loss = round(loss,5)\n",
    "print('Accuracy : ', acc)\n",
    "print('var_loss is : ', var_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "# Clear any logs from previous runs\n",
    "# rm -rf ./logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf_bph')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e4ccb82a3f882ff33793e2fd72a967c991b1009db10b47a306b585d520f8880"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
